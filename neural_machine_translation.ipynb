{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tom\n",
      "[nltk_data]     Lazar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import pickle\n",
    "import data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sentence_size = 30\n",
    "\n",
    "def read_sentences(file_path):\n",
    "    sentences = []\n",
    "\n",
    "    with open(file_path, 'r') as reader:\n",
    "        for s in reader:\n",
    "            sentences.append(s.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def read_all_sentences(file_paths):\n",
    "    all_sentences = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        all_sentences += read_sentences(file_path)\n",
    "\n",
    "    return all_sentences\n",
    "\n",
    "def iteritems(dic):\n",
    "    return iter([(key, dic[key]) for key in dic])\n",
    "\n",
    "stopwords_=set(stopwords.words('english'))\n",
    "\n",
    "def create_dataset(en_sentences, de_sentences):\n",
    "    bad_characters = ',.\" ;:)(][?!|'\n",
    "    \n",
    "    en_sentences = [[word.strip(bad_characters).lower() for word in sentence.split()] for sentence in en_sentences]\n",
    "    de_sentences = [[word.strip(bad_characters).lower() for word in sentence.split()] for sentence in de_sentences]\n",
    "    \n",
    "    take_out_stopwords = False\n",
    "    \n",
    "    en_vocab_dict = Counter(word for sentence in en_sentences for word in sentence if ((not take_out_stopwords or word not in stopwords_) and word is not ''))\n",
    "    de_vocab_dict = Counter(word for sentence in de_sentences for word in sentence if ((not take_out_stopwords or word not in stopwords_) and word is not ''))\n",
    "\n",
    "    en_vocab = list(map(lambda x: x[0], sorted(en_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    de_vocab = list(map(lambda x: x[0], sorted(de_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "\n",
    "    en_vocab = en_vocab[:25000]\n",
    "    de_vocab = de_vocab[:50000]\n",
    "    print(de_vocab[:50])\n",
    "\n",
    "    start_idx = 2\n",
    "    en_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(en_vocab)])\n",
    "    en_word2idx['<ukn>'] = 0\n",
    "    en_word2idx['<pad>'] = 1\n",
    "\n",
    "    en_idx2word = dict([(idx, word) for word, idx in iteritems(en_word2idx)])\n",
    "\n",
    "\n",
    "    start_idx = 4\n",
    "    de_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(de_vocab)])\n",
    "    de_word2idx['<ukn>'] = 0\n",
    "    de_word2idx['<go>']  = 1\n",
    "    de_word2idx['<eos>'] = 2\n",
    "    de_word2idx['<pad>'] = 3\n",
    "\n",
    "    de_idx2word = dict([(idx, word) for word, idx in iteritems(de_word2idx)])\n",
    "\n",
    "    x = [[en_word2idx.get(word.strip(bad_characters), 0) for word in sentence if (not take_out_stopwords or word not in stopwords_)] for sentence in en_sentences]\n",
    "    y = [[de_word2idx.get(word.strip(bad_characters), 0) for word in sentence if (not take_out_stopwords or word not in stopwords_)] for sentence in de_sentences]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2 \n",
    "        #if abs(n1 - n2) <= 0.3 * n:\n",
    "        if n1 <= sentence_size and n2 <= sentence_size:\n",
    "            X.append(x[i])\n",
    "            Y.append(y[i])\n",
    "\n",
    "    return X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "def main():\n",
    "    en_sentences = read_all_sentences(['./Data/bible.en', './Data/Gita-data.en'])\n",
    "    de_sentences = read_all_sentences(['./Data/bible.san', './Data/Gita-data.san'])\n",
    "\n",
    "    save_dataset('./Data/data.pkl', create_dataset(de_sentences, en_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'the', 'of', 'to', 'he', 'that', 'in', 'him', 'unto', 'they', 'is', 'i', 'a', 'for', 'them', 'not', 'be', 'his', 'was', 'with', 'shall', 'said', 'which', 'but', 'it', 'ye', 'you', 'all', 'when', 'me', 'god', 'have', 'are', 'as', 'this', 'jesus', 'thou', 'from', 'man', 'by', 'were', 'into', 'my', 'had', 'on', 'then', 'one', 'there', 'come', 'their']\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sanskrit(uni):\n",
    "    a = bytearray(uni, encoding = \"utf-8\").decode('unicode-escape')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f,encoding=\"utf_8\")\n",
    "\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab = read_dataset('./Data/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in Sanskrit - encoded: [688, 1181, 506, 6, 1181, 11328, 0]\n",
      "Sentence in English - encoded: [5, 583, 6, 5, 462, 6, 39, 139, 0, 5, 62, 6, 333, 0, 5, 62, 6, 328, 0]\n",
      "Decoded:\n",
      "------------------------\n",
      "इब्राहीमः पुत्र इस्हाक् तस्य पुत्रो याकूब् तस्य पुत्रो यिहूदास्तस्य भ्रातरश्च। \n",
      "\n",
      "abraham begat isaac <ukn> and isaac begat jacob <ukn> and jacob begat judas and his brethren <ukn> "
     ]
    }
   ],
   "source": [
    "#inspecting data\n",
    "print('Sentence in Sanskrit - encoded:', X[0])\n",
    "print('Sentence in English - encoded:', Y[0])\n",
    "print('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[1])):\n",
    "    print(convert_sanskrit(en_idx2word[X[1][i]]), end = \" \")\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[1])):\n",
    "    print(de_idx2word[Y[1][i]], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.58403468502933\n",
      "21.631981637337415\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(sentence) for sentence in X]) / len(X))\n",
    "print(sum([len(sentence) for sentence in Y]) / len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3921 3921 3528 393 3528 393\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = sentence_size):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [de_word2idx['<go>']] + y[i] + [de_word2idx['<eos>']] + (length-len(y[i])) * [de_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "print(len(X), len(Y), len(X_train), len(X_test), len(Y_train), len(Y_test))\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = sentence_size\n",
    "output_seq_len = input_seq_len + 2\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "de_vocab_size = len(de_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = de_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tom Lazar\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = de_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = de_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == de_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(de_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_placeholder = tf.placeholder(tf.float32, shape=[])\n",
    "default_learning_rate = 0.01\n",
    "learning_rate = default_learning_rate\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate_placeholder).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate_thresholds = {\n",
    "    10000000: default_learning_rate,\n",
    "    5: 0.008,\n",
    "    2: 0.005,\n",
    "    1: 0.001,\n",
    "    0.05: 0.0008,\n",
    "    0.01: 0.0001,\n",
    "    0.001: 0.00001\n",
    "}\n",
    "batch_size = 728\n",
    "steps = 4000\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, optimizer, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "Restoring\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints501/-399\n",
      "Running from step 400\n",
      "Starting\n",
      "step: 409, loss: 2.682797431945801 learning_rate: 0.005\n",
      "New learning rate\n",
      "step: 419, loss: 2.70666241645813 learning_rate: 0.008\n",
      "step: 429, loss: 2.285507917404175 learning_rate: 0.008\n",
      "step: 439, loss: 2.7030978202819824 learning_rate: 0.008\n",
      "step: 449, loss: 2.633201837539673 learning_rate: 0.008\n",
      "step: 459, loss: 2.433051109313965 learning_rate: 0.008\n",
      "step: 469, loss: 2.5908358097076416 learning_rate: 0.008\n",
      "step: 479, loss: 2.3824462890625 learning_rate: 0.008\n",
      "step: 489, loss: 2.42240571975708 learning_rate: 0.008\n",
      "step: 499, loss: 2.3938679695129395 learning_rate: 0.008\n",
      "Checkpoint is saved\n",
      "step: 509, loss: 2.3565049171447754 learning_rate: 0.008\n",
      "step: 519, loss: 2.2031619548797607 learning_rate: 0.008\n",
      "step: 529, loss: 2.0262017250061035 learning_rate: 0.008\n",
      "step: 539, loss: 1.7325880527496338 learning_rate: 0.008\n",
      "New learning rate\n",
      "step: 549, loss: 1.8218576908111572 learning_rate: 0.005\n",
      "step: 559, loss: 1.936568260192871 learning_rate: 0.005\n",
      "step: 569, loss: 1.7225244045257568 learning_rate: 0.005\n",
      "step: 579, loss: 1.7934951782226562 learning_rate: 0.005\n",
      "step: 589, loss: 1.5785367488861084 learning_rate: 0.005\n",
      "step: 599, loss: 1.6779870986938477 learning_rate: 0.005\n",
      "Checkpoint is saved\n",
      "step: 609, loss: 1.6108405590057373 learning_rate: 0.005\n",
      "step: 619, loss: 1.4656426906585693 learning_rate: 0.005\n",
      "step: 629, loss: 1.6852545738220215 learning_rate: 0.005\n",
      "step: 639, loss: 1.4522639513015747 learning_rate: 0.005\n",
      "step: 649, loss: 1.3100717067718506 learning_rate: 0.005\n",
      "step: 659, loss: 1.2757971286773682 learning_rate: 0.005\n",
      "step: 669, loss: 1.243553876876831 learning_rate: 0.005\n",
      "step: 679, loss: 1.290008783340454 learning_rate: 0.005\n",
      "step: 689, loss: 1.2671339511871338 learning_rate: 0.005\n",
      "step: 699, loss: 1.2696473598480225 learning_rate: 0.005\n",
      "Checkpoint is saved\n",
      "step: 709, loss: 1.202075719833374 learning_rate: 0.005\n",
      "step: 719, loss: 1.1760950088500977 learning_rate: 0.005\n",
      "step: 729, loss: 1.2796123027801514 learning_rate: 0.005\n",
      "step: 739, loss: 1.050168752670288 learning_rate: 0.005\n",
      "step: 749, loss: 1.2820708751678467 learning_rate: 0.005\n",
      "step: 759, loss: 1.1106624603271484 learning_rate: 0.005\n",
      "step: 769, loss: 1.0165138244628906 learning_rate: 0.005\n",
      "step: 779, loss: 0.9970773458480835 learning_rate: 0.005\n",
      "New learning rate\n",
      "step: 789, loss: 1.1521165370941162 learning_rate: 0.001\n",
      "New learning rate\n",
      "step: 799, loss: 1.4045751094818115 learning_rate: 0.005\n",
      "Checkpoint is saved\n",
      "step: 809, loss: 0.9293539524078369 learning_rate: 0.005\n",
      "New learning rate\n",
      "step: 819, loss: 0.9184901118278503 learning_rate: 0.001\n",
      "step: 829, loss: 0.8954491019248962 learning_rate: 0.001\n",
      "step: 839, loss: 1.0607988834381104 learning_rate: 0.001\n",
      "New learning rate\n",
      "step: 849, loss: 0.8664574027061462 learning_rate: 0.005\n",
      "New learning rate\n",
      "step: 859, loss: 0.7564108371734619 learning_rate: 0.001\n",
      "step: 869, loss: 0.7319309711456299 learning_rate: 0.001\n",
      "step: 879, loss: 0.7588295340538025 learning_rate: 0.001\n",
      "step: 889, loss: 0.667104959487915 learning_rate: 0.001\n",
      "step: 899, loss: 0.7665212154388428 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 909, loss: 0.6561204791069031 learning_rate: 0.001\n",
      "step: 919, loss: 0.6996440887451172 learning_rate: 0.001\n",
      "step: 929, loss: 0.7071987390518188 learning_rate: 0.001\n",
      "step: 939, loss: 0.48554742336273193 learning_rate: 0.001\n",
      "step: 949, loss: 0.5553587675094604 learning_rate: 0.001\n",
      "step: 959, loss: 0.5885354280471802 learning_rate: 0.001\n",
      "step: 969, loss: 0.469059020280838 learning_rate: 0.001\n",
      "step: 979, loss: 0.49487030506134033 learning_rate: 0.001\n",
      "step: 989, loss: 0.5684328079223633 learning_rate: 0.001\n",
      "step: 999, loss: 0.48658639192581177 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1009, loss: 0.47447723150253296 learning_rate: 0.001\n",
      "step: 1019, loss: 0.480681836605072 learning_rate: 0.001\n",
      "step: 1029, loss: 0.3512336015701294 learning_rate: 0.001\n",
      "step: 1039, loss: 0.3663704991340637 learning_rate: 0.001\n",
      "step: 1049, loss: 0.38218408823013306 learning_rate: 0.001\n",
      "step: 1059, loss: 0.3477403223514557 learning_rate: 0.001\n",
      "step: 1069, loss: 0.34623658657073975 learning_rate: 0.001\n",
      "step: 1079, loss: 0.32296186685562134 learning_rate: 0.001\n",
      "step: 1089, loss: 0.2767449617385864 learning_rate: 0.001\n",
      "step: 1099, loss: 0.26686832308769226 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1109, loss: 0.2700180411338806 learning_rate: 0.001\n",
      "step: 1119, loss: 0.30259835720062256 learning_rate: 0.001\n",
      "step: 1129, loss: 0.2573879361152649 learning_rate: 0.001\n",
      "step: 1139, loss: 0.2112559825181961 learning_rate: 0.001\n",
      "step: 1149, loss: 0.20300088822841644 learning_rate: 0.001\n",
      "step: 1159, loss: 0.2740691602230072 learning_rate: 0.001\n",
      "step: 1169, loss: 0.20838236808776855 learning_rate: 0.001\n",
      "step: 1179, loss: 0.20121848583221436 learning_rate: 0.001\n",
      "step: 1189, loss: 0.2141263484954834 learning_rate: 0.001\n",
      "step: 1199, loss: 0.21192096173763275 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1209, loss: 0.23862047493457794 learning_rate: 0.001\n",
      "step: 1219, loss: 0.19380947947502136 learning_rate: 0.001\n",
      "step: 1229, loss: 0.18462517857551575 learning_rate: 0.001\n",
      "step: 1239, loss: 0.17908042669296265 learning_rate: 0.001\n",
      "step: 1249, loss: 0.17115765810012817 learning_rate: 0.001\n",
      "step: 1259, loss: 0.14538489282131195 learning_rate: 0.001\n",
      "step: 1269, loss: 0.20304982364177704 learning_rate: 0.001\n",
      "step: 1279, loss: 0.21182101964950562 learning_rate: 0.001\n",
      "step: 1289, loss: 0.1823602318763733 learning_rate: 0.001\n",
      "step: 1299, loss: 0.2045268714427948 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1309, loss: 0.16830110549926758 learning_rate: 0.001\n",
      "step: 1319, loss: 0.12740576267242432 learning_rate: 0.001\n",
      "step: 1329, loss: 0.15515923500061035 learning_rate: 0.001\n",
      "step: 1339, loss: 0.16930684447288513 learning_rate: 0.001\n",
      "step: 1349, loss: 0.18595066666603088 learning_rate: 0.001\n",
      "step: 1359, loss: 0.11206498742103577 learning_rate: 0.001\n",
      "step: 1369, loss: 0.11434352397918701 learning_rate: 0.001\n",
      "step: 1379, loss: 0.09140080958604813 learning_rate: 0.001\n",
      "step: 1389, loss: 0.13245895504951477 learning_rate: 0.001\n",
      "step: 1399, loss: 0.14509636163711548 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1409, loss: 0.10184389352798462 learning_rate: 0.001\n",
      "step: 1419, loss: 0.11775926500558853 learning_rate: 0.001\n",
      "step: 1429, loss: 0.09056706726551056 learning_rate: 0.001\n",
      "step: 1439, loss: 0.09049980342388153 learning_rate: 0.001\n",
      "step: 1449, loss: 0.10863044857978821 learning_rate: 0.001\n",
      "step: 1459, loss: 0.08928373456001282 learning_rate: 0.001\n",
      "step: 1469, loss: 0.07886626571416855 learning_rate: 0.001\n",
      "step: 1479, loss: 0.1252773255109787 learning_rate: 0.001\n",
      "step: 1489, loss: 0.0807855874300003 learning_rate: 0.001\n",
      "step: 1499, loss: 0.12140445411205292 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1509, loss: 0.07286786288022995 learning_rate: 0.001\n",
      "step: 1519, loss: 0.09887861460447311 learning_rate: 0.001\n",
      "step: 1529, loss: 0.09296471625566483 learning_rate: 0.001\n",
      "step: 1539, loss: 0.07459818571805954 learning_rate: 0.001\n",
      "step: 1549, loss: 0.10705104470252991 learning_rate: 0.001\n",
      "step: 1559, loss: 0.10821421444416046 learning_rate: 0.001\n",
      "step: 1569, loss: 0.09620232880115509 learning_rate: 0.001\n",
      "step: 1579, loss: 0.06342533975839615 learning_rate: 0.001\n",
      "step: 1589, loss: 0.07741838693618774 learning_rate: 0.001\n",
      "step: 1599, loss: 0.07523182034492493 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1609, loss: 0.07630085945129395 learning_rate: 0.001\n",
      "step: 1619, loss: 0.09542156755924225 learning_rate: 0.001\n",
      "step: 1629, loss: 0.08936292678117752 learning_rate: 0.001\n",
      "step: 1639, loss: 0.05931805819272995 learning_rate: 0.001\n",
      "step: 1649, loss: 0.08548164367675781 learning_rate: 0.001\n",
      "step: 1659, loss: 0.04762645810842514 learning_rate: 0.001\n",
      "New learning rate\n",
      "step: 1669, loss: 0.050067298114299774 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 1679, loss: 0.07392453402280807 learning_rate: 0.001\n",
      "step: 1689, loss: 0.05790252611041069 learning_rate: 0.001\n",
      "step: 1699, loss: 0.09795751422643661 learning_rate: 0.001\n",
      "Checkpoint is saved\n",
      "step: 1709, loss: 0.05869625508785248 learning_rate: 0.001\n",
      "step: 1719, loss: 0.05536452680826187 learning_rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1729, loss: 0.06684702634811401 learning_rate: 0.001\n",
      "step: 1739, loss: 0.06981673836708069 learning_rate: 0.001\n",
      "step: 1749, loss: 0.1042078360915184 learning_rate: 0.001\n",
      "step: 1759, loss: 0.040401071310043335 learning_rate: 0.001\n",
      "New learning rate\n",
      "step: 1769, loss: 0.04845914989709854 learning_rate: 0.0008\n",
      "step: 1779, loss: 0.0453171506524086 learning_rate: 0.0008\n",
      "step: 1789, loss: 0.03857841715216637 learning_rate: 0.0008\n",
      "step: 1799, loss: 0.05254897475242615 learning_rate: 0.0008\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 1809, loss: 0.08554959297180176 learning_rate: 0.001\n",
      "step: 1819, loss: 0.04202941805124283 learning_rate: 0.001\n",
      "New learning rate\n",
      "step: 1829, loss: 0.018220778554677963 learning_rate: 0.0008\n",
      "step: 1839, loss: 0.025423910468816757 learning_rate: 0.0008\n",
      "step: 1849, loss: 0.06032416224479675 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 1859, loss: 0.06285335123538971 learning_rate: 0.001\n",
      "step: 1869, loss: 0.05171450600028038 learning_rate: 0.001\n",
      "step: 1879, loss: 0.059527114033699036 learning_rate: 0.001\n",
      "step: 1889, loss: 0.05979619920253754 learning_rate: 0.001\n",
      "step: 1899, loss: 0.03801863640546799 learning_rate: 0.001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 1909, loss: 0.024800680577754974 learning_rate: 0.0008\n",
      "step: 1919, loss: 0.041218552738428116 learning_rate: 0.0008\n",
      "step: 1929, loss: 0.02876538224518299 learning_rate: 0.0008\n",
      "step: 1939, loss: 0.02215178683400154 learning_rate: 0.0008\n",
      "step: 1949, loss: 0.03216487169265747 learning_rate: 0.0008\n",
      "step: 1959, loss: 0.029633546248078346 learning_rate: 0.0008\n",
      "step: 1969, loss: 0.036065854132175446 learning_rate: 0.0008\n",
      "step: 1979, loss: 0.013360620476305485 learning_rate: 0.0008\n",
      "step: 1989, loss: 0.023977311328053474 learning_rate: 0.0008\n",
      "step: 1999, loss: 0.020693957805633545 learning_rate: 0.0008\n",
      "Checkpoint is saved\n",
      "step: 2009, loss: 0.019967926666140556 learning_rate: 0.0008\n",
      "step: 2019, loss: 0.0314800925552845 learning_rate: 0.0008\n",
      "step: 2029, loss: 0.04523457586765289 learning_rate: 0.0008\n",
      "step: 2039, loss: 0.019512001425027847 learning_rate: 0.0008\n",
      "step: 2049, loss: 0.047274064272642136 learning_rate: 0.0008\n",
      "step: 2059, loss: 0.05898179113864899 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2069, loss: 0.045875128358602524 learning_rate: 0.001\n",
      "New learning rate\n",
      "step: 2079, loss: 0.01781819574534893 learning_rate: 0.0008\n",
      "step: 2089, loss: 0.02158188261091709 learning_rate: 0.0008\n",
      "step: 2099, loss: 0.03404153883457184 learning_rate: 0.0008\n",
      "Checkpoint is saved\n",
      "step: 2109, loss: 0.019915781915187836 learning_rate: 0.0008\n",
      "step: 2119, loss: 0.017845090478658676 learning_rate: 0.0008\n",
      "step: 2129, loss: 0.018923193216323853 learning_rate: 0.0008\n",
      "step: 2139, loss: 0.018955711275339127 learning_rate: 0.0008\n",
      "step: 2149, loss: 0.019142642617225647 learning_rate: 0.0008\n",
      "step: 2159, loss: 0.04001772403717041 learning_rate: 0.0008\n",
      "step: 2169, loss: 0.026860695332288742 learning_rate: 0.0008\n",
      "step: 2179, loss: 0.029671560972929 learning_rate: 0.0008\n",
      "step: 2189, loss: 0.008028088137507439 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2199, loss: 0.012163805775344372 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 2209, loss: 0.0312054306268692 learning_rate: 0.0008\n",
      "step: 2219, loss: 0.0279875248670578 learning_rate: 0.0008\n",
      "step: 2229, loss: 0.048079196363687515 learning_rate: 0.0008\n",
      "step: 2239, loss: 0.030145494267344475 learning_rate: 0.0008\n",
      "step: 2249, loss: 0.014499780721962452 learning_rate: 0.0008\n",
      "step: 2259, loss: 0.03923628479242325 learning_rate: 0.0008\n",
      "step: 2269, loss: 0.01484779454767704 learning_rate: 0.0008\n",
      "step: 2279, loss: 0.013775952160358429 learning_rate: 0.0008\n",
      "step: 2289, loss: 0.03739767149090767 learning_rate: 0.0008\n",
      "step: 2299, loss: 0.007517059333622456 learning_rate: 0.0008\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 2309, loss: 0.010884666815400124 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 2319, loss: 0.02154446765780449 learning_rate: 0.0008\n",
      "step: 2329, loss: 0.026321077719330788 learning_rate: 0.0008\n",
      "step: 2339, loss: 0.0405401773750782 learning_rate: 0.0008\n",
      "step: 2349, loss: 0.0154346264898777 learning_rate: 0.0008\n",
      "step: 2359, loss: 0.019552666693925858 learning_rate: 0.0008\n",
      "step: 2369, loss: 0.00986439734697342 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2379, loss: 0.013946514576673508 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 2389, loss: 0.018034789711236954 learning_rate: 0.0008\n",
      "step: 2399, loss: 0.04712445288896561 learning_rate: 0.0008\n",
      "Checkpoint is saved\n",
      "step: 2409, loss: 0.027619849890470505 learning_rate: 0.0008\n",
      "step: 2419, loss: 0.021829012781381607 learning_rate: 0.0008\n",
      "step: 2429, loss: 0.008801927790045738 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2439, loss: 0.012941736727952957 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 2449, loss: 0.011555726639926434 learning_rate: 0.0008\n",
      "step: 2459, loss: 0.014225363731384277 learning_rate: 0.0008\n",
      "step: 2469, loss: 0.01521921157836914 learning_rate: 0.0008\n",
      "step: 2479, loss: 0.014776919037103653 learning_rate: 0.0008\n",
      "step: 2489, loss: 0.024020913988351822 learning_rate: 0.0008\n",
      "step: 2499, loss: 0.039622023701667786 learning_rate: 0.0008\n",
      "Checkpoint is saved\n",
      "step: 2509, loss: 0.05284038558602333 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2519, loss: 0.04270297661423683 learning_rate: 0.001\n",
      "New learning rate\n",
      "step: 2529, loss: 0.020911119878292084 learning_rate: 0.0008\n",
      "step: 2539, loss: 0.021238500252366066 learning_rate: 0.0008\n",
      "step: 2549, loss: 0.025211017578840256 learning_rate: 0.0008\n",
      "step: 2559, loss: 0.025828305631875992 learning_rate: 0.0008\n",
      "step: 2569, loss: 0.02275361865758896 learning_rate: 0.0008\n",
      "step: 2579, loss: 0.031828466802835464 learning_rate: 0.0008\n",
      "step: 2589, loss: 0.015525939874351025 learning_rate: 0.0008\n",
      "step: 2599, loss: 0.026686862111091614 learning_rate: 0.0008\n",
      "Checkpoint is saved\n",
      "step: 2609, loss: 0.016972163692116737 learning_rate: 0.0008\n",
      "step: 2619, loss: 0.014655780047178268 learning_rate: 0.0008\n",
      "step: 2629, loss: 0.012480699457228184 learning_rate: 0.0008\n",
      "step: 2639, loss: 0.020456749945878983 learning_rate: 0.0008\n",
      "step: 2649, loss: 0.024097273126244545 learning_rate: 0.0008\n",
      "step: 2659, loss: 0.011513639241456985 learning_rate: 0.0008\n",
      "step: 2669, loss: 0.011533888056874275 learning_rate: 0.0008\n",
      "step: 2679, loss: 0.011417980305850506 learning_rate: 0.0008\n",
      "step: 2689, loss: 0.02743871882557869 learning_rate: 0.0008\n",
      "step: 2699, loss: 0.01729707047343254 learning_rate: 0.0008\n",
      "Checkpoint is saved\n",
      "step: 2709, loss: 0.009060588665306568 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2719, loss: 0.0061565241776406765 learning_rate: 0.0001\n",
      "step: 2729, loss: 0.006276222877204418 learning_rate: 0.0001\n",
      "step: 2739, loss: 0.013624617829918861 learning_rate: 0.0001\n",
      "New learning rate\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b407ff3686c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlearning_rate_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mbackward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m9\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-dc0c3d03eb56>\u001b[0m in \u001b[0;36mbackward_step\u001b[1;34m(sess, optimizer, feed)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbackward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "#tf.trainable_variables())\n",
    "checkpointsPath = './checkpoints501fix/'\n",
    "restore = False\n",
    "starting_step = 0\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if (restore):\n",
    "        print('Restoring')\n",
    "        with open(checkpointsPath + 'checkpoint') as f:\n",
    "            starting_step = int(re.match('model_checkpoint_path: \"-([0-9]+)\"', list(f)[0]).groups()[0]) + 1\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpointsPath))\n",
    "        print('Running from step {}'.format(starting_step))\n",
    "    else:\n",
    "        print('Running from scratch: generating random model parameters.')\n",
    "        learning_rate = default_learning_rate\n",
    "    \n",
    "    print(\"Starting\")\n",
    "    t = time.time()\n",
    "    for step in range(starting_step, starting_step + steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "        feed[learning_rate_placeholder] = learning_rate\n",
    "            \n",
    "        backward_step(sess, optimizer, feed)\n",
    "        \n",
    "        if step % 10 == 9 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {} learning_rate: {}'.format(step, loss_value, learning_rate))\n",
    "            losses.append(loss_value)\n",
    "            \n",
    "            # training op\n",
    "            new_learning_rate = learning_rate_thresholds[sorted(filter(lambda x: x > loss_value, list(learning_rate_thresholds.keys())))[0]]\n",
    "            if new_learning_rate is not learning_rate:\n",
    "                print('New learning rate')\n",
    "                learning_rate = new_learning_rate\n",
    "            \n",
    "        \n",
    "        if step % 100 == 99:\n",
    "            saver.save(sess, checkpointsPath, global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "view = losses[805:]\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    stepinterval = 10\n",
    "    plt.plot(list(range(0, len(view) * stepinterval, stepinterval)), view, linewidth = 1)\n",
    "    plt.xlabel('Training Progress (steps)'.format(starting_step))\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((-0.01, max(view) + (0.25 * max(view))))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints501/-2699\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-d1f4d6ef1720>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mde_word2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<go>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-d1f4d6ef1720>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mde_word2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<go>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = de_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    #let's translate these sentences     \n",
    "#     en_sentences = [\"\\u0907\\u092c\\u094d\\u0930\\u093e\\u0939\\u0940\\u092e\\u0903 \\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u0926\\u093e\\u092f\\u0942\\u0926\\u094d \\u0924\\u0938\\u094d\\u092f \",\\\n",
    "#                     '\\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u092f\\u0940\\u0936\\u0941\\u0916\\u094d\\u0930\\u0940\\u0937\\u094d\\u091f\\u0938\\u094d\\u0924\\u0938\\u094d\\u092f']\n",
    "#     en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    en_sentences_encoded = X_test[10:20]\n",
    "    print(len(X_test))\n",
    "    de_sentences_encoded = Y_test[10:20]\n",
    "        \n",
    "    additional_en_sentences = ['\\\\u0924\\\\u0924\\\\u0903 <ukn> \\\\u092a\\\\u0930\\\\u0902 \\\\u092a\\\\u0941\\\\u0928\\\\u0930\\\\u0928\\\\u094d\\\\u092f\\\\u094b \\\\u091c\\\\u0928\\\\u094b \\\\u0928\\\\u093f\\\\u0936\\\\u094d\\\\u091a\\\\u093f\\\\u0924\\\\u094d\\\\u092f \\\\u092c\\\\u092d\\\\u093e\\\\u0937\\\\u0947']\n",
    "    additional_en_sentences = [sentence + ' '.join(['<pad>'] * (sentence_size - len(sentence.split()))) for sentence in additional_en_sentences]    \n",
    "    en_sentences_encoded += [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in additional_en_sentences]\n",
    "    additional_de_sentences = ['and about the space of one hour after another confidently affirmed']\n",
    "    additional_de_sentences = [sentence + ' '.join(['<pad>'] * (sentence_size - len(sentence.split()))) for sentence in additional_de_sentences]\n",
    "    de_sentences_encoded += [[de_word2idx.get(word, 0) for word in de_sentence.split()] for de_sentence in additional_de_sentences]\n",
    "    \n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint(checkpointsPath)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([de_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print(\" \". join([convert_sanskrit(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            print(\" \". join([(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            print('Translation: ', end = \" \")\n",
    "            for j in range(len(words)):\n",
    "                if words[j] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print((words[j]), end = \" \")\n",
    "            print()\n",
    "            print('   Expected: ', end = \" \")\n",
    "            for j in range(len(de_sentences_encoded[i])):\n",
    "                if de_idx2word[de_sentences_encoded[i][j]] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print((de_idx2word[de_sentences_encoded[i][j]]), end = \" \")\n",
    "            \n",
    "            print('\\n--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model can be improved by using more training steps, better dataset or even with better selection of hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3528\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<go> a new commandment i give unto you <ukn> that ye love one another <ukn> as i have loved you <ukn> that ye also love one another <ukn> <eos> <pad> <pad> <pad>\n",
      "<go> and he said unto them <ukn> full well ye reject the commandment of god <ukn> that ye may keep your own tradition <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and all the people that came together to that sight <ukn> beholding the things which were done <ukn> smote their breasts <ukn> and returned <ukn> <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and how hear we every man in our own tongue <ukn> wherein we were born <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> howbeit jesus spake of his death <ukn> but they thought that he had spoken of taking of rest in sleep <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and the serpent cast out of his mouth water as a flood after the woman <ukn> that he might cause her to be carried away of the flood <ukn> <eos> <pad>\n",
      "<go> he saith unto them <ukn> how then doth david in spirit call him lord <ukn> saying <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and they said unto him <ukn> where wilt thou that we prepare <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> jesus answered and said unto him <ukn> art thou a master of israel <ukn> and knowest not these things <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and james the son of zebedee <ukn> and john the brother of james <ukn> and he surnamed them boanerges <ukn> which is <ukn> the sons of thunder <ukn> <eos> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for s in [' '.join([de_idx2word[word] for word in sentence]) for sentence in Y_test[:10]]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
