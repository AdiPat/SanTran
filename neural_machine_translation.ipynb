{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivanikohli/Documents/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pickle\n",
    "import data_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def read_sentences(file_path):\n",
    "\tsentences = []\n",
    "\n",
    "\twith open(file_path, 'r') as reader:\n",
    "\t\tfor s in reader:\n",
    "\t\t\tsentences.append(s.strip())\n",
    "\n",
    "\treturn sentences\n",
    "\n",
    "def iteritems(dic):\n",
    "    return iter([(key, dic[key]) for key in dic])\n",
    "\n",
    "def create_dataset(en_sentences, de_sentences):\n",
    "\n",
    "\ten_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in en_sentences for word in sentence.split())\n",
    "\tde_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in de_sentences for word in sentence.split())\n",
    "\n",
    "\ten_vocab = list(map(lambda x: x[0], sorted(en_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "\tde_vocab = list(map(lambda x: x[0], sorted(de_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "\n",
    "\ten_vocab = en_vocab[:20000]\n",
    "\tde_vocab = de_vocab[:30000]\n",
    "\n",
    "\tstart_idx = 2\n",
    "\ten_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(en_vocab)])\n",
    "\ten_word2idx['<ukn>'] = 0\n",
    "\ten_word2idx['<pad>'] = 1\n",
    "\n",
    "\ten_idx2word = dict([(idx, word) for word, idx in iteritems(en_word2idx)])\n",
    "\n",
    "\n",
    "\tstart_idx = 4\n",
    "\tde_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(de_vocab)])\n",
    "\tde_word2idx['<ukn>'] = 0\n",
    "\tde_word2idx['<go>']  = 1\n",
    "\tde_word2idx['<eos>'] = 2\n",
    "\tde_word2idx['<pad>'] = 3\n",
    "\n",
    "\tde_idx2word = dict([(idx, word) for word, idx in iteritems(de_word2idx)])\n",
    "\n",
    "\tx = [[en_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in en_sentences]\n",
    "\ty = [[de_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in de_sentences]\n",
    "\n",
    "\tX = []\n",
    "\tY = []\n",
    "\tfor i in range(len(x)):\n",
    "\t\tn1 = len(x[i])\n",
    "\t\tn2 = len(y[i])\n",
    "\t\tn = n1 if n1 < n2 else n2 \n",
    "\t\tif abs(n1 - n2) <= 0.3 * n:\n",
    "\t\t\tif n1 <= 15 and n2 <= 15:\n",
    "\t\t\t\tX.append(x[i])\n",
    "\t\t\t\tY.append(y[i])\n",
    "\n",
    "\treturn X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "\twith open(file_path, 'wb') as f:\n",
    "\t\tpickle.dump(obj, f, -1)\n",
    "\n",
    "def main():\n",
    "    en_sentences = read_sentences('bible.en')\n",
    "    de_sentences = read_sentences('bible.san')\n",
    "\n",
    "    save_dataset('./bible.pkl', create_dataset(en_sentences, de_sentences))\n",
    "    save_dataset('./bible2.pkl', create_dataset(de_sentences, en_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sanskrit(uni):\n",
    "    a = bytearray(uni, encoding = \"utf-8\").decode('unicode-escape')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f,encoding=\"utf_8\")\n",
    "\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab = read_dataset('bible2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in Sanskrit - encoded: [6411]\n",
      "Sentence in English - encoded: [3443]\n",
      "Decoded:\n",
      "------------------------\n",
      "तस्य पुत्रो रिहबियाम्  तस्य पुत्रोऽबियः  तस्य पुत्र आसा  । \n",
      "\n",
      "And Solomon begat Roboam  and Roboam begat Abia  and Abia begat Asa  "
     ]
    }
   ],
   "source": [
    "#inspecting data\n",
    "print('Sentence in Sanskrit - encoded:', X[0])\n",
    "print('Sentence in English - encoded:', Y[0])\n",
    "print('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[1])):\n",
    "    print(convert_sanskrit(en_idx2word[X[1][i]]), end = \" \")\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[1])):\n",
    "    print(de_idx2word[Y[1][i]], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 10):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [de_word2idx['<go>']] + y[i] + [de_word2idx['<eos>']] + (length-len(y[i])) * [de_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 10\n",
    "output_seq_len = 12\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "de_vocab_size = len(de_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = de_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shivanikohli/Documents/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1346: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = de_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = de_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == de_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(de_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "steps = 200\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess=tf.Session()    \n",
    "# #First let's load meta graph and restore weights\n",
    "# saver = tf.train.import_meta_graph('checkpointsSan/-199.meta')\n",
    "# saver.restore(sess,tf.train.latest_checkpoint('checkpointsSan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 7.956083297729492\n",
      "step: 4, loss: 7.965521812438965\n",
      "step: 9, loss: 7.956542491912842\n",
      "step: 14, loss: 7.929145812988281\n",
      "step: 19, loss: 7.925655364990234\n",
      "Checkpoint is saved\n",
      "step: 24, loss: 7.951839447021484\n",
      "step: 29, loss: 7.942840576171875\n",
      "step: 34, loss: 7.9441094398498535\n",
      "step: 39, loss: 7.96900749206543\n",
      "Checkpoint is saved\n",
      "step: 44, loss: 7.916626930236816\n",
      "step: 49, loss: 7.89055061340332\n",
      "step: 54, loss: 7.834952354431152\n",
      "step: 59, loss: 7.74992561340332\n",
      "Checkpoint is saved\n",
      "step: 64, loss: 7.487989902496338\n",
      "step: 69, loss: 6.685154914855957\n",
      "step: 74, loss: 5.780820369720459\n",
      "step: 79, loss: 6.129973411560059\n",
      "Checkpoint is saved\n",
      "step: 84, loss: 4.878608703613281\n",
      "step: 89, loss: 5.039995193481445\n",
      "step: 94, loss: 5.11327600479126\n",
      "step: 99, loss: 4.560495376586914\n",
      "Checkpoint is saved\n",
      "step: 104, loss: 4.040718078613281\n",
      "step: 109, loss: 4.414769649505615\n",
      "step: 114, loss: 5.063276767730713\n",
      "step: 119, loss: 10.50705337524414\n",
      "Checkpoint is saved\n",
      "step: 124, loss: 8.768077850341797\n",
      "step: 129, loss: 3.837125778198242\n",
      "step: 134, loss: 2.581907272338867\n",
      "step: 139, loss: 1.8165953159332275\n",
      "Checkpoint is saved\n",
      "step: 144, loss: 1.960049033164978\n",
      "step: 149, loss: 1.8283787965774536\n",
      "step: 154, loss: 2.1898181438446045\n",
      "step: 159, loss: 1.239960789680481\n",
      "Checkpoint is saved\n",
      "step: 164, loss: 1.1891494989395142\n",
      "step: 169, loss: 0.9188858270645142\n",
      "step: 174, loss: 0.9211083650588989\n",
      "step: 179, loss: 0.15309101343154907\n",
      "Checkpoint is saved\n",
      "step: 184, loss: 0.014839998446404934\n",
      "step: 189, loss: 0.015433917753398418\n",
      "step: 194, loss: 0.0033483533188700676\n",
      "step: 199, loss: 0.25782227516174316\n",
      "Checkpoint is saved\n",
      "Training time for 200 steps: 294.95702385902405s\n"
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "# del saver\n",
    "saver = tf.train.Saver()\n",
    "# restore = True\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "with tf.Session() as sess:\n",
    "#     if (restore):\n",
    "#         saver.restore(sess, tf.train.latest_checkpoint('checkpointsSan/'))\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, 'checkpointsSan2EngLonger/', global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c27ed1f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((-1, 50))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpointsSan2EngLonger/-199\n",
      "1.\n",
      "--------------------------------\n",
      "तद्वद् अन्यशास्त्रेपि लिख्यते  यथा   दृष्टिपातं करिष्यन्ति तेऽविधन् यन्तु तम्प्रति।  <pad> <pad>\n",
      "\\u0924\\u0926\\u094d\\u0935\\u0926\\u094d \\u0905\\u0928\\u094d\\u092f\\u0936\\u093e\\u0938\\u094d\\u0924\\u094d\\u0930\\u0947\\u092a\\u093f \\u0932\\u093f\\u0916\\u094d\\u092f\\u0924\\u0947  \\u092f\\u0925\\u093e   \\u0926\\u0943\\u0937\\u094d\\u091f\\u093f\\u092a\\u093e\\u0924\\u0902 \\u0915\\u0930\\u093f\\u0937\\u094d\\u092f\\u0928\\u094d\\u0924\\u093f \\u0924\\u0947\\u093d\\u0935\\u093f\\u0927\\u0928\\u094d \\u092f\\u0928\\u094d\\u0924\\u0941 \\u0924\\u092e\\u094d\\u092a\\u094d\\u0930\\u0924\\u093f\\u0964  <pad> <pad>\n",
      "And again another scripture saith  They a look on him  And again another scripture saith  They a look on him  \n",
      "--------------------------------\n",
      "2.\n",
      "--------------------------------\n",
      "अपरञ्च भावयित्वा निजायुषः क्षणमात्रं वर्द्धयितुं शक्नोति  एतादृशो लाको युष्माकं मध्ये कोस्ति  <pad> <pad>\n",
      "\\u0905\\u092a\\u0930\\u091e\\u094d\\u091a \\u092d\\u093e\\u0935\\u092f\\u093f\\u0924\\u094d\\u0935\\u093e \\u0928\\u093f\\u091c\\u093e\\u092f\\u0941\\u0937\\u0903 \\u0915\\u094d\\u0937\\u0923\\u092e\\u093e\\u0924\\u094d\\u0930\\u0902 \\u0935\\u0930\\u094d\\u0926\\u094d\\u0927\\u092f\\u093f\\u0924\\u0941\\u0902 \\u0936\\u0915\\u094d\\u0928\\u094b\\u0924\\u093f  \\u090f\\u0924\\u093e\\u0926\\u0943\\u0936\\u094b \\u0932\\u093e\\u0915\\u094b \\u092f\\u0941\\u0937\\u094d\\u092e\\u093e\\u0915\\u0902 \\u092e\\u0927\\u094d\\u092f\\u0947 \\u0915\\u094b\\u0938\\u094d\\u0924\\u093f  <pad> <pad>\n",
      "And which of you with taking thought can add to his thought And which of you with taking thought can add to his thought \n",
      "--------------------------------\n",
      "3.\n",
      "--------------------------------\n",
      "तेषां नामानीमानि  शिमोन् सिवदिपुत्रो <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0947\\u0937\\u093e\\u0902 \\u0928\\u093e\\u092e\\u093e\\u0928\\u0940\\u092e\\u093e\\u0928\\u093f  \\u0936\\u093f\\u092e\\u094b\\u0928\\u094d \\u0938\\u093f\\u0935\\u0926\\u093f\\u092a\\u0941\\u0924\\u094d\\u0930\\u094b <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "And Simon he surnamed surnamed Peter  he first  And Simon he surnamed surnamed Peter  he first  \n",
      "--------------------------------\n",
      "4.\n",
      "--------------------------------\n",
      " <ukn> इदं <ukn> न च <ukn> त्वं मल्कीषेदकः श्रेण्यां याजकोऽसि सदातनः।  <pad> <pad>\n",
      " <ukn> \\u0907\\u0926\\u0902 <ukn> \\u0928 \\u091a <ukn> \\u0924\\u094d\\u0935\\u0902 \\u092e\\u0932\\u094d\\u0915\\u0940\\u0937\\u0947\\u0926\\u0915\\u0903 \\u0936\\u094d\\u0930\\u0947\\u0923\\u094d\\u092f\\u093e\\u0902 \\u092f\\u093e\\u091c\\u0915\\u094b\\u093d\\u0938\\u093f \\u0938\\u0926\\u093e\\u0924\\u0928\\u0903\\u0964  <pad> <pad>\n",
      "By so much was Jesus made a surety of a better a By so much was Jesus made a surety of a better a \n",
      "--------------------------------\n",
      "5.\n",
      "--------------------------------\n",
      "ततो लोका उच्चैःकारं प्रत्यवदन्  एष मनुजरवो न हि  ईश्वरीयरवः। <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0924\\u094b \\u0932\\u094b\\u0915\\u093e \\u0909\\u091a\\u094d\\u091a\\u0948\\u0903\\u0915\\u093e\\u0930\\u0902 \\u092a\\u094d\\u0930\\u0924\\u094d\\u092f\\u0935\\u0926\\u0928\\u094d  \\u090f\\u0937 \\u092e\\u0928\\u0941\\u091c\\u0930\\u0935\\u094b \\u0928 \\u0939\\u093f  \\u0908\\u0936\\u094d\\u0935\\u0930\\u0940\\u092f\\u0930\\u0935\\u0903\\u0964 <pad> <pad> <pad> <pad>\n",
      "But the word of God grew and multiplied  multiplied  But the word of God grew and multiplied  multiplied  \n",
      "--------------------------------\n",
      "6.\n",
      "--------------------------------\n",
      "<ukn> <ukn> तव वंशो <ukn> इति वाग् यमधि कथिता तम् अद्वितीयं पुत्रं <ukn> स <ukn>\n",
      "<ukn> <ukn> \\u0924\\u0935 \\u0935\\u0902\\u0936\\u094b <ukn> \\u0907\\u0924\\u093f \\u0935\\u093e\\u0917\\u094d \\u092f\\u092e\\u0927\\u093f \\u0915\\u0925\\u093f\\u0924\\u093e \\u0924\\u092e\\u094d \\u0905\\u0926\\u094d\\u0935\\u093f\\u0924\\u0940\\u092f\\u0902 \\u092a\\u0941\\u0924\\u094d\\u0930\\u0902 <ukn> \\u0938 <ukn>\n",
      "Of whom it was said  That in Isaac shall thy thy Of whom it was said  That in Isaac shall thy thy \n",
      "--------------------------------\n",
      "7.\n",
      "--------------------------------\n",
      "ते स्वैः सार्द्धं पुनः कतिपयदिनानि स्थातुं तं <ukn>  स <ukn> कथामेतां कथितवान्  <pad>\n",
      "\\u0924\\u0947 \\u0938\\u094d\\u0935\\u0948\\u0903 \\u0938\\u093e\\u0930\\u094d\\u0926\\u094d\\u0927\\u0902 \\u092a\\u0941\\u0928\\u0903 \\u0915\\u0924\\u093f\\u092a\\u092f\\u0926\\u093f\\u0928\\u093e\\u0928\\u093f \\u0938\\u094d\\u0925\\u093e\\u0924\\u0941\\u0902 \\u0924\\u0902 <ukn>  \\u0938 <ukn> \\u0915\\u0925\\u093e\\u092e\\u0947\\u0924\\u093e\\u0902 \\u0915\\u0925\\u093f\\u0924\\u0935\\u093e\\u0928\\u094d  <pad>\n",
      "When they behold him to tarry longer time with them  longer When they behold him to tarry longer time with them  longer \n",
      "--------------------------------\n",
      "8.\n",
      "--------------------------------\n",
      "अनन्तरं यीशुः सायंकाले द्वादशभिः शिष्यैः सार्द्धं जगाम  <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0905\\u0928\\u0928\\u094d\\u0924\\u0930\\u0902 \\u092f\\u0940\\u0936\\u0941\\u0903 \\u0938\\u093e\\u092f\\u0902\\u0915\\u093e\\u0932\\u0947 \\u0926\\u094d\\u0935\\u093e\\u0926\\u0936\\u092d\\u093f\\u0903 \\u0936\\u093f\\u0937\\u094d\\u092f\\u0948\\u0903 \\u0938\\u093e\\u0930\\u094d\\u0926\\u094d\\u0927\\u0902 \\u091c\\u0917\\u093e\\u092e  <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "And in the evening he cometh with the twelve  he And in the evening he cometh with the twelve  he \n",
      "--------------------------------\n",
      "9.\n",
      "--------------------------------\n",
      "ततः सर्व्वे लोकास्तं गमनागमने कुर्व्वन्तम् ईश्वरं धन्यं वदन्तञ्च विलोक्य <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0924\\u0903 \\u0938\\u0930\\u094d\\u0935\\u094d\\u0935\\u0947 \\u0932\\u094b\\u0915\\u093e\\u0938\\u094d\\u0924\\u0902 \\u0917\\u092e\\u0928\\u093e\\u0917\\u092e\\u0928\\u0947 \\u0915\\u0941\\u0930\\u094d\\u0935\\u094d\\u0935\\u0928\\u094d\\u0924\\u092e\\u094d \\u0908\\u0936\\u094d\\u0935\\u0930\\u0902 \\u0927\\u0928\\u094d\\u092f\\u0902 \\u0935\\u0926\\u0928\\u094d\\u0924\\u091e\\u094d\\u091a \\u0935\\u093f\\u0932\\u094b\\u0915\\u094d\\u092f <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "And all the people saw him and praising praising praising God  And all the people saw him and praising praising praising God  \n",
      "--------------------------------\n",
      "10.\n",
      "--------------------------------\n",
      "हे मम भ्रातरः  संसारो यदि युष्मान् द्वेष्टि तर्हि तद् आश्चर्य्यं न मन्यध्वं। <pad> <pad>\n",
      "\\u0939\\u0947 \\u092e\\u092e \\u092d\\u094d\\u0930\\u093e\\u0924\\u0930\\u0903  \\u0938\\u0902\\u0938\\u093e\\u0930\\u094b \\u092f\\u0926\\u093f \\u092f\\u0941\\u0937\\u094d\\u092e\\u093e\\u0928\\u094d \\u0926\\u094d\\u0935\\u0947\\u0937\\u094d\\u091f\\u093f \\u0924\\u0930\\u094d\\u0939\\u093f \\u0924\\u0926\\u094d \\u0906\\u0936\\u094d\\u091a\\u0930\\u094d\\u092f\\u094d\\u092f\\u0902 \\u0928 \\u092e\\u0928\\u094d\\u092f\\u0927\\u094d\\u0935\\u0902\\u0964 <pad> <pad>\n",
      "Marvel not  my brethren  if a world hate world hate Marvel not  my brethren  if a world hate world hate \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = de_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    #let's translate these sentences     \n",
    "#     en_sentences = [\"\\u0907\\u092c\\u094d\\u0930\\u093e\\u0939\\u0940\\u092e\\u0903 \\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u0926\\u093e\\u092f\\u0942\\u0926\\u094d \\u0924\\u0938\\u094d\\u092f \",\\\n",
    "#                     '\\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u092f\\u0940\\u0936\\u0941\\u0916\\u094d\\u0930\\u0940\\u0937\\u094d\\u091f\\u0938\\u094d\\u0924\\u0938\\u094d\\u092f']\n",
    "#     en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    en_sentences_encoded = X_train[:10]\n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('checkpointsSan2EngLonger')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([de_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print(\" \". join([convert_sanskrit(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            print(\" \". join([(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print((words[i]), end = \" \")\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print(words[i], end = \" \")\n",
    "            \n",
    "            print('\\n--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model can be improved by using more training steps, better dataset or even with better selection of hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
