{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tom\n",
      "[nltk_data]     Lazar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import pickle\n",
    "import data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sentence_size = 30\n",
    "\n",
    "def read_sentences(file_path):\n",
    "    sentences = []\n",
    "\n",
    "    with open(file_path, 'r') as reader:\n",
    "        for s in reader:\n",
    "            sentences.append(s.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def read_all_sentences(file_paths):\n",
    "    all_sentences = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        all_sentences += read_sentences(file_path)\n",
    "\n",
    "    return all_sentences\n",
    "\n",
    "def iteritems(dic):\n",
    "    return iter([(key, dic[key]) for key in dic])\n",
    "\n",
    "stopwords_=set(stopwords.words('english'))\n",
    "\n",
    "def create_dataset(en_sentences, de_sentences):\n",
    "    bad_characters = ',.\" ;:)(][?!|'\n",
    "    \n",
    "    en_sentences = [[word.strip(bad_characters).lower() for word in sentence.split()] for sentence in en_sentences]\n",
    "    de_sentences = [[word.strip(bad_characters).lower() for word in sentence.split()] for sentence in de_sentences]\n",
    "    \n",
    "    take_out_stopwords = False\n",
    "    \n",
    "    en_vocab_dict = Counter(word for sentence in en_sentences for word in sentence if ((not take_out_stopwords or word not in stopwords_) and word is not ''))\n",
    "    de_vocab_dict = Counter(word for sentence in de_sentences for word in sentence if ((not take_out_stopwords or word not in stopwords_) and word is not ''))\n",
    "\n",
    "    en_vocab = list(map(lambda x: x[0], sorted(en_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    de_vocab = list(map(lambda x: x[0], sorted(de_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "\n",
    "    en_vocab = en_vocab[:25000]\n",
    "    de_vocab = de_vocab[:50000]\n",
    "    print(de_vocab[:50])\n",
    "\n",
    "    start_idx = 2\n",
    "    en_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(en_vocab)])\n",
    "    en_word2idx['<ukn>'] = 0\n",
    "    en_word2idx['<pad>'] = 1\n",
    "\n",
    "    en_idx2word = dict([(idx, word) for word, idx in iteritems(en_word2idx)])\n",
    "\n",
    "\n",
    "    start_idx = 4\n",
    "    de_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(de_vocab)])\n",
    "    de_word2idx['<ukn>'] = 0\n",
    "    de_word2idx['<go>']  = 1\n",
    "    de_word2idx['<eos>'] = 2\n",
    "    de_word2idx['<pad>'] = 3\n",
    "\n",
    "    de_idx2word = dict([(idx, word) for word, idx in iteritems(de_word2idx)])\n",
    "\n",
    "    x = [[en_word2idx.get(word.strip(bad_characters), 0) for word in sentence if (not take_out_stopwords or word not in stopwords_)] for sentence in en_sentences]\n",
    "    y = [[de_word2idx.get(word.strip(bad_characters), 0) for word in sentence if (not take_out_stopwords or word not in stopwords_)] for sentence in de_sentences]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2 \n",
    "        #if abs(n1 - n2) <= 0.3 * n:\n",
    "        if n1 <= sentence_size and n2 <= sentence_size:\n",
    "            X.append(x[i])\n",
    "            Y.append(y[i])\n",
    "\n",
    "    return X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "def main():\n",
    "    en_sentences = read_all_sentences(['./Data/bible.en', './Data/Gita-data.en'])\n",
    "    de_sentences = read_all_sentences(['./Data/bible.san', './Data/Gita-data.san'])\n",
    "\n",
    "    save_dataset('./Data/data.pkl', create_dataset(de_sentences, en_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'the', 'of', 'to', 'he', 'that', 'in', 'him', 'unto', 'they', 'is', 'i', 'a', 'for', 'them', 'not', 'be', 'his', 'was', 'with', 'shall', 'said', 'which', 'but', 'it', 'ye', 'you', 'all', 'when', 'me', 'god', 'have', 'are', 'as', 'this', 'jesus', 'thou', 'from', 'man', 'by', 'were', 'into', 'my', 'had', 'on', 'then', 'one', 'there', 'come', 'their']\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sanskrit(uni):\n",
    "    a = bytearray(uni, encoding = \"utf-8\").decode('unicode-escape')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f,encoding=\"utf_8\")\n",
    "\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab = read_dataset('./Data/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in Sanskrit - encoded: [688, 1181, 506, 6, 1181, 11328, 0]\n",
      "Sentence in English - encoded: [5, 583, 6, 5, 462, 6, 39, 139, 0, 5, 62, 6, 333, 0, 5, 62, 6, 328, 0]\n",
      "Decoded:\n",
      "------------------------\n",
      "इब्राहीमः पुत्र इस्हाक् तस्य पुत्रो याकूब् तस्य पुत्रो यिहूदास्तस्य भ्रातरश्च। \n",
      "\n",
      "abraham begat isaac <ukn> and isaac begat jacob <ukn> and jacob begat judas and his brethren <ukn> "
     ]
    }
   ],
   "source": [
    "#inspecting data\n",
    "print('Sentence in Sanskrit - encoded:', X[0])\n",
    "print('Sentence in English - encoded:', Y[0])\n",
    "print('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[1])):\n",
    "    print(convert_sanskrit(en_idx2word[X[1][i]]), end = \" \")\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[1])):\n",
    "    print(de_idx2word[Y[1][i]], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.58403468502933\n",
      "21.631981637337415\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(sentence) for sentence in X]) / len(X))\n",
    "print(sum([len(sentence) for sentence in Y]) / len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3921 3921 3528 393 3528 393\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = sentence_size):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [de_word2idx['<go>']] + y[i] + [de_word2idx['<eos>']] + (length-len(y[i])) * [de_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "print(len(X), len(Y), len(X_train), len(X_test), len(Y_train), len(Y_test))\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = sentence_size\n",
    "output_seq_len = input_seq_len + 2\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "de_vocab_size = len(de_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = de_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tom Lazar\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = de_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = de_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == de_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(de_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_placeholder = tf.placeholder(tf.float32, shape=[])\n",
    "default_learning_rate = 0.01\n",
    "learning_rate = default_learning_rate\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate_placeholder).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate_thresholds = {\n",
    "    10000000: default_learning_rate,\n",
    "    5: 0.008,\n",
    "    2: 0.005,\n",
    "    1: 0.001,\n",
    "    0.05: 0.0008,\n",
    "    0.01: 0.0001,\n",
    "    0.001: 0.00001\n",
    "}\n",
    "batch_size = 728\n",
    "steps = 4000\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, optimizer, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "Restoring\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints501/-2699\n",
      "Running from step 2700\n",
      "Starting\n",
      "step: 2709, loss: 0.02000393345952034 learning_rate: 0.0008\n",
      "step: 2719, loss: 0.03420315310359001 learning_rate: 0.0008\n",
      "step: 2729, loss: 0.015851590782403946 learning_rate: 0.0008\n",
      "step: 2739, loss: 0.024697119370102882 learning_rate: 0.0008\n",
      "step: 2749, loss: 0.026792652904987335 learning_rate: 0.0008\n",
      "step: 2759, loss: 0.012472307309508324 learning_rate: 0.0008\n",
      "step: 2769, loss: 0.01267612911760807 learning_rate: 0.0008\n",
      "step: 2779, loss: 0.018158234655857086 learning_rate: 0.0008\n",
      "step: 2789, loss: 0.010731488466262817 learning_rate: 0.0008\n",
      "step: 2799, loss: 0.01920895464718342 learning_rate: 0.0008\n",
      "Checkpoint is saved\n",
      "step: 2809, loss: 0.013034552335739136 learning_rate: 0.0008\n",
      "step: 2819, loss: 0.008359741419553757 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2829, loss: 0.017493868246674538 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 2839, loss: 0.005033544264733791 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2849, loss: 0.010568248108029366 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 2859, loss: 0.005036689806729555 learning_rate: 0.0008\n",
      "New learning rate\n",
      "step: 2869, loss: 0.008014888502657413 learning_rate: 0.0001\n",
      "step: 2879, loss: 0.005763538181781769 learning_rate: 0.0001\n",
      "step: 2889, loss: 0.005338413640856743 learning_rate: 0.0001\n",
      "step: 2899, loss: 0.006931465119123459 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 2909, loss: 0.004979816731065512 learning_rate: 0.0001\n",
      "step: 2919, loss: 0.004905162379145622 learning_rate: 0.0001\n",
      "step: 2929, loss: 0.006238944362848997 learning_rate: 0.0001\n",
      "step: 2939, loss: 0.005061948671936989 learning_rate: 0.0001\n",
      "step: 2949, loss: 0.003880742471665144 learning_rate: 0.0001\n",
      "step: 2959, loss: 0.004409840330481529 learning_rate: 0.0001\n",
      "step: 2969, loss: 0.004062383435666561 learning_rate: 0.0001\n",
      "step: 2979, loss: 0.00513071333989501 learning_rate: 0.0001\n",
      "step: 2989, loss: 0.0034913611598312855 learning_rate: 0.0001\n",
      "step: 2999, loss: 0.007479717954993248 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3009, loss: 0.0024913125671446323 learning_rate: 0.0001\n",
      "step: 3019, loss: 0.004191999789327383 learning_rate: 0.0001\n",
      "step: 3029, loss: 0.004188790917396545 learning_rate: 0.0001\n",
      "step: 3039, loss: 0.00517061073333025 learning_rate: 0.0001\n",
      "step: 3049, loss: 0.005792542360723019 learning_rate: 0.0001\n",
      "step: 3059, loss: 0.0038360983598977327 learning_rate: 0.0001\n",
      "step: 3069, loss: 0.0020474204793572426 learning_rate: 0.0001\n",
      "step: 3079, loss: 0.004840747453272343 learning_rate: 0.0001\n",
      "step: 3089, loss: 0.002391891088336706 learning_rate: 0.0001\n",
      "step: 3099, loss: 0.002978987991809845 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3109, loss: 0.0035754097625613213 learning_rate: 0.0001\n",
      "step: 3119, loss: 0.0022562039084732533 learning_rate: 0.0001\n",
      "step: 3129, loss: 0.0023502823896706104 learning_rate: 0.0001\n",
      "step: 3139, loss: 0.0033513479866087437 learning_rate: 0.0001\n",
      "step: 3149, loss: 0.00290498323738575 learning_rate: 0.0001\n",
      "step: 3159, loss: 0.0033191910479217768 learning_rate: 0.0001\n",
      "step: 3169, loss: 0.0020526929292827845 learning_rate: 0.0001\n",
      "step: 3179, loss: 0.005589806474745274 learning_rate: 0.0001\n",
      "step: 3189, loss: 0.002880138112232089 learning_rate: 0.0001\n",
      "step: 3199, loss: 0.006748671643435955 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3209, loss: 0.0031390616204589605 learning_rate: 0.0001\n",
      "step: 3219, loss: 0.002290165051817894 learning_rate: 0.0001\n",
      "step: 3229, loss: 0.0021676188334822655 learning_rate: 0.0001\n",
      "step: 3239, loss: 0.002033673692494631 learning_rate: 0.0001\n",
      "step: 3249, loss: 0.0019879718311131 learning_rate: 0.0001\n",
      "step: 3259, loss: 0.002170563442632556 learning_rate: 0.0001\n",
      "step: 3269, loss: 0.003943407908082008 learning_rate: 0.0001\n",
      "step: 3279, loss: 0.002467239974066615 learning_rate: 0.0001\n",
      "step: 3289, loss: 0.0016188116278499365 learning_rate: 0.0001\n",
      "step: 3299, loss: 0.0015627789543941617 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3309, loss: 0.00199550180695951 learning_rate: 0.0001\n",
      "step: 3319, loss: 0.0013777354033663869 learning_rate: 0.0001\n",
      "step: 3329, loss: 0.0016654266510158777 learning_rate: 0.0001\n",
      "step: 3339, loss: 0.0025789416395127773 learning_rate: 0.0001\n",
      "step: 3349, loss: 0.0023826616816222668 learning_rate: 0.0001\n",
      "step: 3359, loss: 0.0017255006823688745 learning_rate: 0.0001\n",
      "step: 3369, loss: 0.0020129848271608353 learning_rate: 0.0001\n",
      "step: 3379, loss: 0.0025624714326113462 learning_rate: 0.0001\n",
      "step: 3389, loss: 0.0035046350676566362 learning_rate: 0.0001\n",
      "step: 3399, loss: 0.0016729503404349089 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3409, loss: 0.0019415972055867314 learning_rate: 0.0001\n",
      "step: 3419, loss: 0.0023231455124914646 learning_rate: 0.0001\n",
      "step: 3429, loss: 0.0012520644813776016 learning_rate: 0.0001\n",
      "step: 3439, loss: 0.0020385566167533398 learning_rate: 0.0001\n",
      "step: 3449, loss: 0.001588950166478753 learning_rate: 0.0001\n",
      "step: 3459, loss: 0.0015080661978572607 learning_rate: 0.0001\n",
      "step: 3469, loss: 0.0018042970914393663 learning_rate: 0.0001\n",
      "step: 3479, loss: 0.003213635180145502 learning_rate: 0.0001\n",
      "step: 3489, loss: 0.0019030403345823288 learning_rate: 0.0001\n",
      "step: 3499, loss: 0.001624267315492034 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3509, loss: 0.0014438643120229244 learning_rate: 0.0001\n",
      "step: 3519, loss: 0.00189649045933038 learning_rate: 0.0001\n",
      "step: 3529, loss: 0.0014606991317123175 learning_rate: 0.0001\n",
      "step: 3539, loss: 0.0014857103815302253 learning_rate: 0.0001\n",
      "step: 3549, loss: 0.0014114666264504194 learning_rate: 0.0001\n",
      "step: 3559, loss: 0.0023685984779149294 learning_rate: 0.0001\n",
      "step: 3569, loss: 0.0026792380958795547 learning_rate: 0.0001\n",
      "step: 3579, loss: 0.0012671761214733124 learning_rate: 0.0001\n",
      "step: 3589, loss: 0.001177130383439362 learning_rate: 0.0001\n",
      "step: 3599, loss: 0.001703543122857809 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3609, loss: 0.0012401245767250657 learning_rate: 0.0001\n",
      "step: 3619, loss: 0.00188905606046319 learning_rate: 0.0001\n",
      "step: 3629, loss: 0.0014424575492739677 learning_rate: 0.0001\n",
      "step: 3639, loss: 0.0015571749536320567 learning_rate: 0.0001\n",
      "step: 3649, loss: 0.001206865650601685 learning_rate: 0.0001\n",
      "step: 3659, loss: 0.001157637918367982 learning_rate: 0.0001\n",
      "step: 3669, loss: 0.0011352666188031435 learning_rate: 0.0001\n",
      "step: 3679, loss: 0.0011341841891407967 learning_rate: 0.0001\n",
      "step: 3689, loss: 0.0016678636893630028 learning_rate: 0.0001\n",
      "step: 3699, loss: 0.002342426683753729 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3709, loss: 0.0019084741361439228 learning_rate: 0.0001\n",
      "step: 3719, loss: 0.002364262705668807 learning_rate: 0.0001\n",
      "step: 3729, loss: 0.003004027996212244 learning_rate: 0.0001\n",
      "step: 3739, loss: 0.0010614660568535328 learning_rate: 0.0001\n",
      "step: 3749, loss: 0.0038779196329414845 learning_rate: 0.0001\n",
      "step: 3759, loss: 0.001160660875029862 learning_rate: 0.0001\n",
      "step: 3769, loss: 0.0010768070351332426 learning_rate: 0.0001\n",
      "step: 3779, loss: 0.0024588010273873806 learning_rate: 0.0001\n",
      "step: 3789, loss: 0.0010560767259448767 learning_rate: 0.0001\n",
      "step: 3799, loss: 0.0016385229537263513 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3809, loss: 0.000918592035304755 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 3819, loss: 0.001687559299170971 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 3829, loss: 0.0011459015076979995 learning_rate: 0.0001\n",
      "step: 3839, loss: 0.0036401674151420593 learning_rate: 0.0001\n",
      "step: 3849, loss: 0.0013276899699121714 learning_rate: 0.0001\n",
      "step: 3859, loss: 0.0009648929699324071 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 3869, loss: 0.0010588342556729913 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 3879, loss: 0.0013663799036294222 learning_rate: 0.0001\n",
      "step: 3889, loss: 0.001143018715083599 learning_rate: 0.0001\n",
      "step: 3899, loss: 0.0015608929097652435 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 3909, loss: 0.0010209856554865837 learning_rate: 0.0001\n",
      "step: 3919, loss: 0.0011881384998559952 learning_rate: 0.0001\n",
      "step: 3929, loss: 0.001501825638115406 learning_rate: 0.0001\n",
      "step: 3939, loss: 0.0014119313564151525 learning_rate: 0.0001\n",
      "step: 3949, loss: 0.0023755410220474005 learning_rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3959, loss: 0.0013379252050071955 learning_rate: 0.0001\n",
      "step: 3969, loss: 0.0009463580208830535 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 3979, loss: 0.0016268892213702202 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 3989, loss: 0.0017058546654880047 learning_rate: 0.0001\n",
      "step: 3999, loss: 0.0008155073737725616 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 4009, loss: 0.0013407866936177015 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4019, loss: 0.0008459403179585934 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4029, loss: 0.0016126577975228429 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4039, loss: 0.0013862201012670994 learning_rate: 0.0001\n",
      "step: 4049, loss: 0.001000592135824263 learning_rate: 0.0001\n",
      "step: 4059, loss: 0.0008946084417402744 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4069, loss: 0.0012300368398427963 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4079, loss: 0.0011150063946843147 learning_rate: 0.0001\n",
      "step: 4089, loss: 0.000783599098213017 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4099, loss: 0.0012695721816271544 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 4109, loss: 0.0027389952447265387 learning_rate: 0.0001\n",
      "step: 4119, loss: 0.0010345978662371635 learning_rate: 0.0001\n",
      "step: 4129, loss: 0.0008529193000867963 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4139, loss: 0.0013536917977035046 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4149, loss: 0.001388567965477705 learning_rate: 0.0001\n",
      "step: 4159, loss: 0.0007459102780558169 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4169, loss: 0.0008625221671536565 learning_rate: 1e-05\n",
      "step: 4179, loss: 0.0032942977268248796 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4189, loss: 0.0008159236167557538 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4199, loss: 0.000899548816960305 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 4209, loss: 0.0011560979764908552 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4219, loss: 0.000875055615324527 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4229, loss: 0.002272501355037093 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4239, loss: 0.0008444114355370402 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4249, loss: 0.0008777478942647576 learning_rate: 1e-05\n",
      "step: 4259, loss: 0.0009241658262908459 learning_rate: 1e-05\n",
      "step: 4269, loss: 0.0010466864332556725 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4279, loss: 0.0008080325787886977 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4289, loss: 0.0009797601960599422 learning_rate: 1e-05\n",
      "step: 4299, loss: 0.0013288343325257301 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 4309, loss: 0.001002112403512001 learning_rate: 0.0001\n",
      "step: 4319, loss: 0.0010367296636104584 learning_rate: 0.0001\n",
      "step: 4329, loss: 0.000796743028331548 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4339, loss: 0.0009461282170377672 learning_rate: 1e-05\n",
      "step: 4349, loss: 0.0012679527280852199 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4359, loss: 0.0008430676534771919 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4369, loss: 0.0008126305765472353 learning_rate: 1e-05\n",
      "step: 4379, loss: 0.0008661107858642936 learning_rate: 1e-05\n",
      "step: 4389, loss: 0.0010014395229518414 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4399, loss: 0.0007949871360324323 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 4409, loss: 0.0007070067222230136 learning_rate: 1e-05\n",
      "step: 4419, loss: 0.0007441581110469997 learning_rate: 1e-05\n",
      "step: 4429, loss: 0.0024414772633463144 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4439, loss: 0.0007765203481540084 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4449, loss: 0.001065761549398303 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4459, loss: 0.0009977519512176514 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4469, loss: 0.002392024267464876 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4479, loss: 0.0008331085555255413 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4489, loss: 0.0009279580553993583 learning_rate: 1e-05\n",
      "step: 4499, loss: 0.0014850808074697852 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 4509, loss: 0.0006809719488956034 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4519, loss: 0.001923535717651248 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4529, loss: 0.000756329798605293 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4539, loss: 0.0013627407606691122 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4549, loss: 0.0026328107342123985 learning_rate: 0.0001\n",
      "step: 4559, loss: 0.0008022854453884065 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4569, loss: 0.0008727345848456025 learning_rate: 1e-05\n",
      "step: 4579, loss: 0.0013360411394387484 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4589, loss: 0.0008762006182223558 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4599, loss: 0.0008417534409090877 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 4609, loss: 0.0008281149202957749 learning_rate: 1e-05\n",
      "step: 4619, loss: 0.000777163659222424 learning_rate: 1e-05\n",
      "step: 4629, loss: 0.00218735309317708 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4639, loss: 0.0007105927215889096 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4649, loss: 0.0008305993396788836 learning_rate: 1e-05\n",
      "step: 4659, loss: 0.0009256751509383321 learning_rate: 1e-05\n",
      "step: 4669, loss: 0.0013855670113116503 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4679, loss: 0.0014122852589935064 learning_rate: 0.0001\n",
      "step: 4689, loss: 0.0007683972362428904 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4699, loss: 0.0008209346560761333 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 4709, loss: 0.0011433226754888892 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4719, loss: 0.0008931913762353361 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4729, loss: 0.0007244639564305544 learning_rate: 1e-05\n",
      "step: 4739, loss: 0.0008981004357337952 learning_rate: 1e-05\n",
      "step: 4749, loss: 0.0006968259112909436 learning_rate: 1e-05\n",
      "step: 4759, loss: 0.0012426471803337336 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4769, loss: 0.000873302633408457 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4779, loss: 0.0009024642640724778 learning_rate: 1e-05\n",
      "step: 4789, loss: 0.0012784962309524417 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4799, loss: 0.0007667618338018656 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 4809, loss: 0.0021674467716366053 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4819, loss: 0.0010127825662493706 learning_rate: 0.0001\n",
      "step: 4829, loss: 0.0006552710547111928 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4839, loss: 0.0007557028438895941 learning_rate: 1e-05\n",
      "step: 4849, loss: 0.0009883777238428593 learning_rate: 1e-05\n",
      "step: 4859, loss: 0.0006967236986383796 learning_rate: 1e-05\n",
      "step: 4869, loss: 0.0007927734404802322 learning_rate: 1e-05\n",
      "step: 4879, loss: 0.0010082221124321222 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4889, loss: 0.0007285253377631307 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4899, loss: 0.0006329506286419928 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 4909, loss: 0.0008121280698105693 learning_rate: 1e-05\n",
      "step: 4919, loss: 0.0006354457000270486 learning_rate: 1e-05\n",
      "step: 4929, loss: 0.0019035942386835814 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4939, loss: 0.0006716822972521186 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4949, loss: 0.002411981113255024 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4959, loss: 0.0009312795591540635 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 4969, loss: 0.0007865217048674822 learning_rate: 1e-05\n",
      "step: 4979, loss: 0.0009477641433477402 learning_rate: 1e-05\n",
      "step: 4989, loss: 0.0011543912114575505 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 4999, loss: 0.0006157145835459232 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 5009, loss: 0.0006708109867759049 learning_rate: 1e-05\n",
      "step: 5019, loss: 0.0006975805154070258 learning_rate: 1e-05\n",
      "step: 5029, loss: 0.0007245674496516585 learning_rate: 1e-05\n",
      "step: 5039, loss: 0.0008632828830741346 learning_rate: 1e-05\n",
      "step: 5049, loss: 0.0007476190803572536 learning_rate: 1e-05\n",
      "step: 5059, loss: 0.0006748640444129705 learning_rate: 1e-05\n",
      "step: 5069, loss: 0.0014597162371501327 learning_rate: 1e-05\n",
      "New learning rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5079, loss: 0.001019881572574377 learning_rate: 0.0001\n",
      "step: 5089, loss: 0.0012195913586765528 learning_rate: 0.0001\n",
      "step: 5099, loss: 0.0011887531727552414 learning_rate: 0.0001\n",
      "Checkpoint is saved\n",
      "step: 5109, loss: 0.0008702684426680207 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5119, loss: 0.0009866447653621435 learning_rate: 1e-05\n",
      "step: 5129, loss: 0.0007538945646956563 learning_rate: 1e-05\n",
      "step: 5139, loss: 0.0017976461676880717 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5149, loss: 0.0007564965635538101 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5159, loss: 0.0012351691257208586 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5169, loss: 0.002533134538680315 learning_rate: 0.0001\n",
      "step: 5179, loss: 0.0007282914011739194 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5189, loss: 0.0006250391015782952 learning_rate: 1e-05\n",
      "step: 5199, loss: 0.0013557660859078169 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 5209, loss: 0.0006723648402839899 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5219, loss: 0.0006923211039975286 learning_rate: 1e-05\n",
      "step: 5229, loss: 0.001966575626283884 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5239, loss: 0.000587803777307272 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5249, loss: 0.0007922389195300639 learning_rate: 1e-05\n",
      "step: 5259, loss: 0.0007720603025518358 learning_rate: 1e-05\n",
      "step: 5269, loss: 0.000810439174529165 learning_rate: 1e-05\n",
      "step: 5279, loss: 0.0008400348597206175 learning_rate: 1e-05\n",
      "step: 5289, loss: 0.0008263132185675204 learning_rate: 1e-05\n",
      "step: 5299, loss: 0.0006290597375482321 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 5309, loss: 0.0023718061856925488 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5319, loss: 0.0006178180919960141 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5329, loss: 0.0033793291077017784 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5339, loss: 0.0006866151234135032 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5349, loss: 0.0006843217415735126 learning_rate: 1e-05\n",
      "step: 5359, loss: 0.0010336032137274742 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5369, loss: 0.000853795325383544 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5379, loss: 0.0009290819289162755 learning_rate: 1e-05\n",
      "step: 5389, loss: 0.0007653920911252499 learning_rate: 1e-05\n",
      "step: 5399, loss: 0.0009313614573329687 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 5409, loss: 0.000868059229105711 learning_rate: 1e-05\n",
      "step: 5419, loss: 0.0005865943385288119 learning_rate: 1e-05\n",
      "step: 5429, loss: 0.0005912721971981227 learning_rate: 1e-05\n",
      "step: 5439, loss: 0.0006439100252464414 learning_rate: 1e-05\n",
      "step: 5449, loss: 0.0010218555107712746 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5459, loss: 0.0007551483577117324 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5469, loss: 0.0014953345526009798 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5479, loss: 0.0009788648458197713 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5489, loss: 0.0007471985882148147 learning_rate: 1e-05\n",
      "step: 5499, loss: 0.002892551012337208 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 5509, loss: 0.0010252776555716991 learning_rate: 0.0001\n",
      "step: 5519, loss: 0.0005701426416635513 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5529, loss: 0.001001246040686965 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5539, loss: 0.0005990664358250797 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5549, loss: 0.0010224705329164863 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5559, loss: 0.000676201656460762 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5569, loss: 0.0006475931731984019 learning_rate: 1e-05\n",
      "step: 5579, loss: 0.0007497139740735292 learning_rate: 1e-05\n",
      "step: 5589, loss: 0.0006272343453019857 learning_rate: 1e-05\n",
      "step: 5599, loss: 0.0007268046610988677 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 5609, loss: 0.0007979042711667717 learning_rate: 1e-05\n",
      "step: 5619, loss: 0.0006465559126809239 learning_rate: 1e-05\n",
      "step: 5629, loss: 0.0007234937511384487 learning_rate: 1e-05\n",
      "step: 5639, loss: 0.0006632623844780028 learning_rate: 1e-05\n",
      "step: 5649, loss: 0.00069868826540187 learning_rate: 1e-05\n",
      "step: 5659, loss: 0.0006712020840495825 learning_rate: 1e-05\n",
      "step: 5669, loss: 0.0006330157048068941 learning_rate: 1e-05\n",
      "step: 5679, loss: 0.001532781752757728 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5689, loss: 0.0005453232442960143 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5699, loss: 0.000734357803594321 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 5709, loss: 0.0006781889824196696 learning_rate: 1e-05\n",
      "step: 5719, loss: 0.0006427631014958024 learning_rate: 1e-05\n",
      "step: 5729, loss: 0.0011879741214215755 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5739, loss: 0.0006388170295394957 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5749, loss: 0.0007140239467844367 learning_rate: 1e-05\n",
      "step: 5759, loss: 0.0009674583561718464 learning_rate: 1e-05\n",
      "step: 5769, loss: 0.000544438196811825 learning_rate: 1e-05\n",
      "step: 5779, loss: 0.0005557507975026965 learning_rate: 1e-05\n",
      "step: 5789, loss: 0.0009840796701610088 learning_rate: 1e-05\n",
      "step: 5799, loss: 0.0008629537187516689 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 5809, loss: 0.0005714785656891763 learning_rate: 1e-05\n",
      "step: 5819, loss: 0.0016235543880611658 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5829, loss: 0.0006114027346484363 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5839, loss: 0.0005977178807370365 learning_rate: 1e-05\n",
      "step: 5849, loss: 0.0010136882774531841 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5859, loss: 0.0007634041248820722 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5869, loss: 0.0006346667651087046 learning_rate: 1e-05\n",
      "step: 5879, loss: 0.0012918738648295403 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5889, loss: 0.00048337012412957847 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5899, loss: 0.0006181281059980392 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 5909, loss: 0.0006362650892697275 learning_rate: 1e-05\n",
      "step: 5919, loss: 0.0007683445001021028 learning_rate: 1e-05\n",
      "step: 5929, loss: 0.0007741996087133884 learning_rate: 1e-05\n",
      "step: 5939, loss: 0.0005368503043428063 learning_rate: 1e-05\n",
      "step: 5949, loss: 0.0006882285233587027 learning_rate: 1e-05\n",
      "step: 5959, loss: 0.0013439791509881616 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 5969, loss: 0.0005979762645438313 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 5979, loss: 0.0006558989407494664 learning_rate: 1e-05\n",
      "step: 5989, loss: 0.0005198969738557935 learning_rate: 1e-05\n",
      "step: 5999, loss: 0.0005527521716430783 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 6009, loss: 0.0005488838069140911 learning_rate: 1e-05\n",
      "step: 6019, loss: 0.000770698650740087 learning_rate: 1e-05\n",
      "step: 6029, loss: 0.0006809603073634207 learning_rate: 1e-05\n",
      "step: 6039, loss: 0.0006164275691844523 learning_rate: 1e-05\n",
      "step: 6049, loss: 0.0007382882758975029 learning_rate: 1e-05\n",
      "step: 6059, loss: 0.0009073569672182202 learning_rate: 1e-05\n",
      "step: 6069, loss: 0.0012592822313308716 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6079, loss: 0.000589445699006319 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6089, loss: 0.0005474057397805154 learning_rate: 1e-05\n",
      "step: 6099, loss: 0.0005836011259816587 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 6109, loss: 0.0006798705435357988 learning_rate: 1e-05\n",
      "step: 6119, loss: 0.000667576037812978 learning_rate: 1e-05\n",
      "step: 6129, loss: 0.0022030428517609835 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6139, loss: 0.0005967209581285715 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6149, loss: 0.0006823213770985603 learning_rate: 1e-05\n",
      "step: 6159, loss: 0.0012165842344984412 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6169, loss: 0.0005482792621478438 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6179, loss: 0.000570245087146759 learning_rate: 1e-05\n",
      "step: 6189, loss: 0.000773125677369535 learning_rate: 1e-05\n",
      "step: 6199, loss: 0.0009384212316945195 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 6209, loss: 0.0008777251932770014 learning_rate: 1e-05\n",
      "step: 6219, loss: 0.0009477828280068934 learning_rate: 1e-05\n",
      "step: 6229, loss: 0.0004570711753331125 learning_rate: 1e-05\n",
      "step: 6239, loss: 0.0006544945063069463 learning_rate: 1e-05\n",
      "step: 6249, loss: 0.0008598158019594848 learning_rate: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6259, loss: 0.0006022226298227906 learning_rate: 1e-05\n",
      "step: 6269, loss: 0.000665049534291029 learning_rate: 1e-05\n",
      "step: 6279, loss: 0.0007771249511279166 learning_rate: 1e-05\n",
      "step: 6289, loss: 0.0007128853467293084 learning_rate: 1e-05\n",
      "step: 6299, loss: 0.0006134063587523997 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 6309, loss: 0.0006377220270223916 learning_rate: 1e-05\n",
      "step: 6319, loss: 0.0005788016133010387 learning_rate: 1e-05\n",
      "step: 6329, loss: 0.001864157384261489 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6339, loss: 0.0006527616060338914 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6349, loss: 0.0007147090509533882 learning_rate: 1e-05\n",
      "step: 6359, loss: 0.0005625861813314259 learning_rate: 1e-05\n",
      "step: 6369, loss: 0.0005857327487319708 learning_rate: 1e-05\n",
      "step: 6379, loss: 0.0006161949131637812 learning_rate: 1e-05\n",
      "step: 6389, loss: 0.0005540936253964901 learning_rate: 1e-05\n",
      "step: 6399, loss: 0.004117777571082115 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 6409, loss: 0.0006768586463294923 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6419, loss: 0.0007264552405104041 learning_rate: 1e-05\n",
      "step: 6429, loss: 0.0005550173809751868 learning_rate: 1e-05\n",
      "step: 6439, loss: 0.0016301581636071205 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6449, loss: 0.0006066551432013512 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6459, loss: 0.0008328261319547892 learning_rate: 1e-05\n",
      "step: 6469, loss: 0.0006620754720643163 learning_rate: 1e-05\n",
      "step: 6479, loss: 0.0007804521010257304 learning_rate: 1e-05\n",
      "step: 6489, loss: 0.000713343673851341 learning_rate: 1e-05\n",
      "step: 6499, loss: 0.004070976749062538 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 6509, loss: 0.0005834689363837242 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6519, loss: 0.0010260073468089104 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6529, loss: 0.000646537693683058 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6539, loss: 0.000633142190054059 learning_rate: 1e-05\n",
      "step: 6549, loss: 0.0007392637780867517 learning_rate: 1e-05\n",
      "step: 6559, loss: 0.0005843535764142871 learning_rate: 1e-05\n",
      "step: 6569, loss: 0.0005795126780867577 learning_rate: 1e-05\n",
      "step: 6579, loss: 0.0007209258619695902 learning_rate: 1e-05\n",
      "step: 6589, loss: 0.0011151861399412155 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6599, loss: 0.000544169859495014 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 6609, loss: 0.0010458044707775116 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6619, loss: 0.0005511560593731701 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6629, loss: 0.0006472718087024987 learning_rate: 1e-05\n",
      "step: 6639, loss: 0.0006321432301774621 learning_rate: 1e-05\n",
      "step: 6649, loss: 0.0006243028910830617 learning_rate: 1e-05\n",
      "step: 6659, loss: 0.0005974628729745746 learning_rate: 1e-05\n",
      "step: 6669, loss: 0.0006914659752510488 learning_rate: 1e-05\n",
      "step: 6679, loss: 0.001203478081151843 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6689, loss: 0.0007673276122659445 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6699, loss: 0.0007928633131086826 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 6709, loss: 0.0006013809470459819 learning_rate: 1e-05\n",
      "step: 6719, loss: 0.0005590684013441205 learning_rate: 1e-05\n",
      "step: 6729, loss: 0.0006851427024230361 learning_rate: 1e-05\n",
      "step: 6739, loss: 0.0005859205266460776 learning_rate: 1e-05\n",
      "step: 6749, loss: 0.001012533437460661 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6759, loss: 0.000757083180360496 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6769, loss: 0.0006321438122540712 learning_rate: 1e-05\n",
      "step: 6779, loss: 0.0007069128332659602 learning_rate: 1e-05\n",
      "step: 6789, loss: 0.0005789175047539175 learning_rate: 1e-05\n",
      "step: 6799, loss: 0.000716696260496974 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 6809, loss: 0.0007097964407876134 learning_rate: 1e-05\n",
      "step: 6819, loss: 0.0017843006644397974 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6829, loss: 0.0008378932252526283 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6839, loss: 0.0009851192589849234 learning_rate: 1e-05\n",
      "step: 6849, loss: 0.0013206249568611383 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 6859, loss: 0.0005921540432609618 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 6869, loss: 0.0004938436904922128 learning_rate: 1e-05\n",
      "step: 6879, loss: 0.0006276312051340938 learning_rate: 1e-05\n",
      "step: 6889, loss: 0.000632133916951716 learning_rate: 1e-05\n",
      "step: 6899, loss: 0.0006460995646193624 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 6909, loss: 0.0005016399663873017 learning_rate: 1e-05\n",
      "step: 6919, loss: 0.0006006844341754913 learning_rate: 1e-05\n",
      "step: 6929, loss: 0.0005745051894336939 learning_rate: 1e-05\n",
      "step: 6939, loss: 0.0005359497154131532 learning_rate: 1e-05\n",
      "step: 6949, loss: 0.000829805270768702 learning_rate: 1e-05\n",
      "step: 6959, loss: 0.0008935744408518076 learning_rate: 1e-05\n",
      "step: 6969, loss: 0.0005394920590333641 learning_rate: 1e-05\n",
      "step: 6979, loss: 0.0005017074290663004 learning_rate: 1e-05\n",
      "step: 6989, loss: 0.0006319178501144052 learning_rate: 1e-05\n",
      "step: 6999, loss: 0.0005247083026915789 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 7009, loss: 0.0005879388190805912 learning_rate: 1e-05\n",
      "step: 7019, loss: 0.0005692214472219348 learning_rate: 1e-05\n",
      "step: 7029, loss: 0.0004489271668717265 learning_rate: 1e-05\n",
      "step: 7039, loss: 0.0007551732705906034 learning_rate: 1e-05\n",
      "step: 7049, loss: 0.0005122377770021558 learning_rate: 1e-05\n",
      "step: 7059, loss: 0.0007184671703726053 learning_rate: 1e-05\n",
      "step: 7069, loss: 0.0014678132720291615 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7079, loss: 0.0005475235520862043 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7089, loss: 0.0006304257549345493 learning_rate: 1e-05\n",
      "step: 7099, loss: 0.0005343618104234338 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 7109, loss: 0.0017478070221841335 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7119, loss: 0.0006201736396178603 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7129, loss: 0.0005319433403201401 learning_rate: 1e-05\n",
      "step: 7139, loss: 0.000949858920648694 learning_rate: 1e-05\n",
      "step: 7149, loss: 0.0005817736382596195 learning_rate: 1e-05\n",
      "step: 7159, loss: 0.00044374028220772743 learning_rate: 1e-05\n",
      "step: 7169, loss: 0.0006056970451027155 learning_rate: 1e-05\n",
      "step: 7179, loss: 0.0006786592421121895 learning_rate: 1e-05\n",
      "step: 7189, loss: 0.0005978897679597139 learning_rate: 1e-05\n",
      "step: 7199, loss: 0.0005240391474217176 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 7209, loss: 0.0054003349505364895 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7219, loss: 0.000596853846218437 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7229, loss: 0.0005365220131352544 learning_rate: 1e-05\n",
      "step: 7239, loss: 0.0038074336480349302 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7249, loss: 0.0006104693165980279 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7259, loss: 0.0005453338380903006 learning_rate: 1e-05\n",
      "step: 7269, loss: 0.0010938055347651243 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7279, loss: 0.0007468671537935734 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7289, loss: 0.0006464365287683904 learning_rate: 1e-05\n",
      "step: 7299, loss: 0.000651980226393789 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 7309, loss: 0.0006618194747716188 learning_rate: 1e-05\n",
      "step: 7319, loss: 0.00043108550016768277 learning_rate: 1e-05\n",
      "step: 7329, loss: 0.0005731186829507351 learning_rate: 1e-05\n",
      "step: 7339, loss: 0.0005370810977183282 learning_rate: 1e-05\n",
      "step: 7349, loss: 0.00046936224680393934 learning_rate: 1e-05\n",
      "step: 7359, loss: 0.002656231401488185 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7369, loss: 0.0009232172742486 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7379, loss: 0.0009834945667535067 learning_rate: 1e-05\n",
      "step: 7389, loss: 0.0004502548254095018 learning_rate: 1e-05\n",
      "step: 7399, loss: 0.000601436011493206 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 7409, loss: 0.0005887991283088923 learning_rate: 1e-05\n",
      "step: 7419, loss: 0.0005459903040900826 learning_rate: 1e-05\n",
      "step: 7429, loss: 0.000986472237855196 learning_rate: 1e-05\n",
      "step: 7439, loss: 0.000685059349052608 learning_rate: 1e-05\n",
      "step: 7449, loss: 0.0007383138872683048 learning_rate: 1e-05\n",
      "step: 7459, loss: 0.0015769398305565119 learning_rate: 1e-05\n",
      "New learning rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7469, loss: 0.0006547527154907584 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7479, loss: 0.002246641553938389 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7489, loss: 0.0014283170457929373 learning_rate: 0.0001\n",
      "step: 7499, loss: 0.000910691567696631 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 7509, loss: 0.00047814295976422727 learning_rate: 1e-05\n",
      "step: 7519, loss: 0.0010754824616014957 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7529, loss: 0.001693126279860735 learning_rate: 0.0001\n",
      "step: 7539, loss: 0.0006802788120694458 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7549, loss: 0.000921956729143858 learning_rate: 1e-05\n",
      "step: 7559, loss: 0.0005745756789110601 learning_rate: 1e-05\n",
      "step: 7569, loss: 0.0004945322871208191 learning_rate: 1e-05\n",
      "step: 7579, loss: 0.0007700618589296937 learning_rate: 1e-05\n",
      "step: 7589, loss: 0.0005041180411353707 learning_rate: 1e-05\n",
      "step: 7599, loss: 0.001611565938219428 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 7609, loss: 0.0006199527997523546 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7619, loss: 0.0008176653063856065 learning_rate: 1e-05\n",
      "step: 7629, loss: 0.0011326472740620375 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7639, loss: 0.0007322106976062059 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7649, loss: 0.0004923593951389194 learning_rate: 1e-05\n",
      "step: 7659, loss: 0.0006505321944132447 learning_rate: 1e-05\n",
      "step: 7669, loss: 0.0006809465121477842 learning_rate: 1e-05\n",
      "step: 7679, loss: 0.0012012270744889975 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7689, loss: 0.0004490537685342133 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7699, loss: 0.0004882274952251464 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 7709, loss: 0.005024509038776159 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7719, loss: 0.0004961799131706357 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7729, loss: 0.0006845392053946853 learning_rate: 1e-05\n",
      "step: 7739, loss: 0.00040803791489452124 learning_rate: 1e-05\n",
      "step: 7749, loss: 0.00040172351873479784 learning_rate: 1e-05\n",
      "step: 7759, loss: 0.0007000787300057709 learning_rate: 1e-05\n",
      "step: 7769, loss: 0.0005001501413062215 learning_rate: 1e-05\n",
      "step: 7779, loss: 0.0004675066447816789 learning_rate: 1e-05\n",
      "step: 7789, loss: 0.0005584995960816741 learning_rate: 1e-05\n",
      "step: 7799, loss: 0.0011209197109565139 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 7809, loss: 0.0009556759614497423 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7819, loss: 0.0004695043317042291 learning_rate: 1e-05\n",
      "step: 7829, loss: 0.0006538432789966464 learning_rate: 1e-05\n",
      "step: 7839, loss: 0.00053969188593328 learning_rate: 1e-05\n",
      "step: 7849, loss: 0.0005234514246694744 learning_rate: 1e-05\n",
      "step: 7859, loss: 0.00047182856360450387 learning_rate: 1e-05\n",
      "step: 7869, loss: 0.0009324345737695694 learning_rate: 1e-05\n",
      "step: 7879, loss: 0.0005319475312717259 learning_rate: 1e-05\n",
      "step: 7889, loss: 0.0004969091969542205 learning_rate: 1e-05\n",
      "step: 7899, loss: 0.0004728097701445222 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 7909, loss: 0.0012327085714787245 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 7919, loss: 0.0004907670663669705 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 7929, loss: 0.0005235816934145987 learning_rate: 1e-05\n",
      "step: 7939, loss: 0.0004331576346885413 learning_rate: 1e-05\n",
      "step: 7949, loss: 0.0004481278592720628 learning_rate: 1e-05\n",
      "step: 7959, loss: 0.0004515706386882812 learning_rate: 1e-05\n",
      "step: 7969, loss: 0.0004283734015189111 learning_rate: 1e-05\n",
      "step: 7979, loss: 0.0005808570422232151 learning_rate: 1e-05\n",
      "step: 7989, loss: 0.00047815972357057035 learning_rate: 1e-05\n",
      "step: 7999, loss: 0.0004836144798900932 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8009, loss: 0.00042448038584552705 learning_rate: 1e-05\n",
      "step: 8019, loss: 0.0004200714756734669 learning_rate: 1e-05\n",
      "step: 8029, loss: 0.0006423519807867706 learning_rate: 1e-05\n",
      "step: 8039, loss: 0.0016912752762436867 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8049, loss: 0.0004766914644278586 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8059, loss: 0.0005553330993279815 learning_rate: 1e-05\n",
      "step: 8069, loss: 0.0006006751209497452 learning_rate: 1e-05\n",
      "step: 8079, loss: 0.0006063601467758417 learning_rate: 1e-05\n",
      "step: 8089, loss: 0.0006283971015363932 learning_rate: 1e-05\n",
      "step: 8099, loss: 0.00047028850531205535 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8109, loss: 0.000612263975199312 learning_rate: 1e-05\n",
      "step: 8119, loss: 0.0011455953354015946 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8129, loss: 0.0004618793318513781 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8139, loss: 0.0004583040135912597 learning_rate: 1e-05\n",
      "step: 8149, loss: 0.00044093935866840184 learning_rate: 1e-05\n",
      "step: 8159, loss: 0.0005265803774818778 learning_rate: 1e-05\n",
      "step: 8169, loss: 0.0006088197696954012 learning_rate: 1e-05\n",
      "step: 8179, loss: 0.00045304675586521626 learning_rate: 1e-05\n",
      "step: 8189, loss: 0.0004608644521795213 learning_rate: 1e-05\n",
      "step: 8199, loss: 0.0005432557663880289 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8209, loss: 0.0006892077508382499 learning_rate: 1e-05\n",
      "step: 8219, loss: 0.0010152864269912243 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8229, loss: 0.00042617606231942773 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8239, loss: 0.0004351524112280458 learning_rate: 1e-05\n",
      "step: 8249, loss: 0.0009896804112941027 learning_rate: 1e-05\n",
      "step: 8259, loss: 0.00042821269016712904 learning_rate: 1e-05\n",
      "step: 8269, loss: 0.0016786674968898296 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8279, loss: 0.00045971732470206916 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8289, loss: 0.0005459111416712403 learning_rate: 1e-05\n",
      "step: 8299, loss: 0.0005637425929307938 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8309, loss: 0.0004483164520934224 learning_rate: 1e-05\n",
      "step: 8319, loss: 0.00043178332271054387 learning_rate: 1e-05\n",
      "step: 8329, loss: 0.0005058869137428701 learning_rate: 1e-05\n",
      "step: 8339, loss: 0.0004523198585957289 learning_rate: 1e-05\n",
      "step: 8349, loss: 0.0005535000236704946 learning_rate: 1e-05\n",
      "step: 8359, loss: 0.0006586379604414105 learning_rate: 1e-05\n",
      "step: 8369, loss: 0.0009426667238585651 learning_rate: 1e-05\n",
      "step: 8379, loss: 0.0005038342787884176 learning_rate: 1e-05\n",
      "step: 8389, loss: 0.0005268100067041814 learning_rate: 1e-05\n",
      "step: 8399, loss: 0.00082774693146348 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8409, loss: 0.0004867520183324814 learning_rate: 1e-05\n",
      "step: 8419, loss: 0.0005520822014659643 learning_rate: 1e-05\n",
      "step: 8429, loss: 0.0005049853352829814 learning_rate: 1e-05\n",
      "step: 8439, loss: 0.0006918178405612707 learning_rate: 1e-05\n",
      "step: 8449, loss: 0.0006437940173782408 learning_rate: 1e-05\n",
      "step: 8459, loss: 0.0007967972778715193 learning_rate: 1e-05\n",
      "step: 8469, loss: 0.0006054238183423877 learning_rate: 1e-05\n",
      "step: 8479, loss: 0.00047125451965257525 learning_rate: 1e-05\n",
      "step: 8489, loss: 0.0004534877953119576 learning_rate: 1e-05\n",
      "step: 8499, loss: 0.001086644595488906 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 8509, loss: 0.0004019414191134274 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8519, loss: 0.0005609114887192845 learning_rate: 1e-05\n",
      "step: 8529, loss: 0.00044731481466442347 learning_rate: 1e-05\n",
      "step: 8539, loss: 0.00042646145448088646 learning_rate: 1e-05\n",
      "step: 8549, loss: 0.0004618563107214868 learning_rate: 1e-05\n",
      "step: 8559, loss: 0.0005281489575281739 learning_rate: 1e-05\n",
      "step: 8569, loss: 0.0005041977856308222 learning_rate: 1e-05\n",
      "step: 8579, loss: 0.0004934367025271058 learning_rate: 1e-05\n",
      "step: 8589, loss: 0.0007565872510895133 learning_rate: 1e-05\n",
      "step: 8599, loss: 0.0004047567490488291 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8609, loss: 0.0007064817473292351 learning_rate: 1e-05\n",
      "step: 8619, loss: 0.00047432241262868047 learning_rate: 1e-05\n",
      "step: 8629, loss: 0.0005096502136439085 learning_rate: 1e-05\n",
      "step: 8639, loss: 0.0004607141308952123 learning_rate: 1e-05\n",
      "step: 8649, loss: 0.0007741471636109054 learning_rate: 1e-05\n",
      "step: 8659, loss: 0.00044014633749611676 learning_rate: 1e-05\n",
      "step: 8669, loss: 0.00042608665535226464 learning_rate: 1e-05\n",
      "step: 8679, loss: 0.0005226118373684585 learning_rate: 1e-05\n",
      "step: 8689, loss: 0.00041268207132816315 learning_rate: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8699, loss: 0.0004998326767235994 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8709, loss: 0.0004197827074676752 learning_rate: 1e-05\n",
      "step: 8719, loss: 0.0010815770365297794 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8729, loss: 0.0005491552292369306 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8739, loss: 0.0011839369544759393 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8749, loss: 0.000768040947150439 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8759, loss: 0.0017748124664649367 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8769, loss: 0.00043480872409418225 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8779, loss: 0.0004628828028216958 learning_rate: 1e-05\n",
      "step: 8789, loss: 0.0005963384755887091 learning_rate: 1e-05\n",
      "step: 8799, loss: 0.00044445061939768493 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8809, loss: 0.001380071509629488 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 8819, loss: 0.0007867400418035686 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 8829, loss: 0.0004133296897634864 learning_rate: 1e-05\n",
      "step: 8839, loss: 0.00043792396900244057 learning_rate: 1e-05\n",
      "step: 8849, loss: 0.0004337269929237664 learning_rate: 1e-05\n",
      "step: 8859, loss: 0.0003884948091581464 learning_rate: 1e-05\n",
      "step: 8869, loss: 0.0005834742914885283 learning_rate: 1e-05\n",
      "step: 8879, loss: 0.00042294961167499423 learning_rate: 1e-05\n",
      "step: 8889, loss: 0.0007637633243575692 learning_rate: 1e-05\n",
      "step: 8899, loss: 0.000407727318815887 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 8909, loss: 0.0005129458731971681 learning_rate: 1e-05\n",
      "step: 8919, loss: 0.00045985120232217014 learning_rate: 1e-05\n",
      "step: 8929, loss: 0.0004571434110403061 learning_rate: 1e-05\n",
      "step: 8939, loss: 0.0005242645274847746 learning_rate: 1e-05\n",
      "step: 8949, loss: 0.00046916501014493406 learning_rate: 1e-05\n",
      "step: 8959, loss: 0.000578573381062597 learning_rate: 1e-05\n",
      "step: 8969, loss: 0.0005710443365387619 learning_rate: 1e-05\n",
      "step: 8979, loss: 0.0004445211379788816 learning_rate: 1e-05\n",
      "step: 8989, loss: 0.0006312470650300384 learning_rate: 1e-05\n",
      "step: 8999, loss: 0.00046154699521139264 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9009, loss: 0.0008745250524953008 learning_rate: 1e-05\n",
      "step: 9019, loss: 0.0005224725464358926 learning_rate: 1e-05\n",
      "step: 9029, loss: 0.0005168532952666283 learning_rate: 1e-05\n",
      "step: 9039, loss: 0.0005223272019065917 learning_rate: 1e-05\n",
      "step: 9049, loss: 0.0006211004219949245 learning_rate: 1e-05\n",
      "step: 9059, loss: 0.00042754760943353176 learning_rate: 1e-05\n",
      "step: 9069, loss: 0.0007793580880388618 learning_rate: 1e-05\n",
      "step: 9079, loss: 0.0004733409150503576 learning_rate: 1e-05\n",
      "step: 9089, loss: 0.0004641854320652783 learning_rate: 1e-05\n",
      "step: 9099, loss: 0.0003858432173728943 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9109, loss: 0.000655060401186347 learning_rate: 1e-05\n",
      "step: 9119, loss: 0.0005266618099994957 learning_rate: 1e-05\n",
      "step: 9129, loss: 0.0016743690939620137 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 9139, loss: 0.00040943201747722924 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 9149, loss: 0.0004235155647620559 learning_rate: 1e-05\n",
      "step: 9159, loss: 0.0004875198064837605 learning_rate: 1e-05\n",
      "step: 9169, loss: 0.00042185332858935 learning_rate: 1e-05\n",
      "step: 9179, loss: 0.00040570611599832773 learning_rate: 1e-05\n",
      "step: 9189, loss: 0.0010028101969510317 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 9199, loss: 0.0008336333557963371 learning_rate: 0.0001\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 9209, loss: 0.0006759370444342494 learning_rate: 1e-05\n",
      "step: 9219, loss: 0.0004812884726561606 learning_rate: 1e-05\n",
      "step: 9229, loss: 0.0005217832513153553 learning_rate: 1e-05\n",
      "step: 9239, loss: 0.00043051346438005567 learning_rate: 1e-05\n",
      "step: 9249, loss: 0.0004123401886317879 learning_rate: 1e-05\n",
      "step: 9259, loss: 0.0006790559273213148 learning_rate: 1e-05\n",
      "step: 9269, loss: 0.00046378865954466164 learning_rate: 1e-05\n",
      "step: 9279, loss: 0.0005238938028924167 learning_rate: 1e-05\n",
      "step: 9289, loss: 0.0005167645285837352 learning_rate: 1e-05\n",
      "step: 9299, loss: 0.00041699601570144296 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9309, loss: 0.0006260756636038423 learning_rate: 1e-05\n",
      "step: 9319, loss: 0.0008056855876930058 learning_rate: 1e-05\n",
      "step: 9329, loss: 0.0005979187553748488 learning_rate: 1e-05\n",
      "step: 9339, loss: 0.0004676772514358163 learning_rate: 1e-05\n",
      "step: 9349, loss: 0.00048195943236351013 learning_rate: 1e-05\n",
      "step: 9359, loss: 0.00043226289562880993 learning_rate: 1e-05\n",
      "step: 9369, loss: 0.0004886864917352796 learning_rate: 1e-05\n",
      "step: 9379, loss: 0.0004122332320548594 learning_rate: 1e-05\n",
      "step: 9389, loss: 0.0004156932409387082 learning_rate: 1e-05\n",
      "step: 9399, loss: 0.0005214622942730784 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9409, loss: 0.00037366559263318777 learning_rate: 1e-05\n",
      "step: 9419, loss: 0.0005134753882884979 learning_rate: 1e-05\n",
      "step: 9429, loss: 0.00047302088933065534 learning_rate: 1e-05\n",
      "step: 9439, loss: 0.0009557574521750212 learning_rate: 1e-05\n",
      "step: 9449, loss: 0.00039731539436616004 learning_rate: 1e-05\n",
      "step: 9459, loss: 0.0005330938729457557 learning_rate: 1e-05\n",
      "step: 9469, loss: 0.000494331878144294 learning_rate: 1e-05\n",
      "step: 9479, loss: 0.00045362519449554384 learning_rate: 1e-05\n",
      "step: 9489, loss: 0.0004341518506407738 learning_rate: 1e-05\n",
      "step: 9499, loss: 0.0004265996103640646 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9509, loss: 0.0010800863383337855 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 9519, loss: 0.00044785786303691566 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 9529, loss: 0.0005462359404191375 learning_rate: 1e-05\n",
      "step: 9539, loss: 0.00048697899910621345 learning_rate: 1e-05\n",
      "step: 9549, loss: 0.0006814022781327367 learning_rate: 1e-05\n",
      "step: 9559, loss: 0.0004694527597166598 learning_rate: 1e-05\n",
      "step: 9569, loss: 0.00046225590631365776 learning_rate: 1e-05\n",
      "step: 9579, loss: 0.0006753706838935614 learning_rate: 1e-05\n",
      "step: 9589, loss: 0.0004172736080363393 learning_rate: 1e-05\n",
      "step: 9599, loss: 0.0007494016317650676 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9609, loss: 0.0004346974892541766 learning_rate: 1e-05\n",
      "step: 9619, loss: 0.00045964939636178315 learning_rate: 1e-05\n",
      "step: 9629, loss: 0.0006698373472318053 learning_rate: 1e-05\n",
      "step: 9639, loss: 0.00047395119327120483 learning_rate: 1e-05\n",
      "step: 9649, loss: 0.00048261642223224044 learning_rate: 1e-05\n",
      "step: 9659, loss: 0.0004968986031599343 learning_rate: 1e-05\n",
      "step: 9669, loss: 0.0020339535549283028 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 9679, loss: 0.00041701417649164796 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 9689, loss: 0.00043104251381009817 learning_rate: 1e-05\n",
      "step: 9699, loss: 0.0010409841779619455 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 9709, loss: 0.0004908897681161761 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 9719, loss: 0.0007120940717868507 learning_rate: 1e-05\n",
      "step: 9729, loss: 0.0008344884845428169 learning_rate: 1e-05\n",
      "step: 9739, loss: 0.0009184330701828003 learning_rate: 1e-05\n",
      "step: 9749, loss: 0.0007560587837360799 learning_rate: 1e-05\n",
      "step: 9759, loss: 0.00040069594979286194 learning_rate: 1e-05\n",
      "step: 9769, loss: 0.0004263875598553568 learning_rate: 1e-05\n",
      "step: 9779, loss: 0.00045112252701073885 learning_rate: 1e-05\n",
      "step: 9789, loss: 0.0004124693223275244 learning_rate: 1e-05\n",
      "step: 9799, loss: 0.00039564387407153845 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9809, loss: 0.00038600910920649767 learning_rate: 1e-05\n",
      "step: 9819, loss: 0.0005468531744554639 learning_rate: 1e-05\n",
      "step: 9829, loss: 0.00040978993638418615 learning_rate: 1e-05\n",
      "step: 9839, loss: 0.0004793922998942435 learning_rate: 1e-05\n",
      "step: 9849, loss: 0.0005297793541103601 learning_rate: 1e-05\n",
      "step: 9859, loss: 0.0004997003125026822 learning_rate: 1e-05\n",
      "step: 9869, loss: 0.00059601163957268 learning_rate: 1e-05\n",
      "step: 9879, loss: 0.0004469109117053449 learning_rate: 1e-05\n",
      "step: 9889, loss: 0.000654124072752893 learning_rate: 1e-05\n",
      "step: 9899, loss: 0.00039459147956222296 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 9909, loss: 0.00044291550875641406 learning_rate: 1e-05\n",
      "step: 9919, loss: 0.0004187029553577304 learning_rate: 1e-05\n",
      "step: 9929, loss: 0.0004692654765676707 learning_rate: 1e-05\n",
      "step: 9939, loss: 0.0004902697401121259 learning_rate: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9949, loss: 0.0010043969377875328 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 9959, loss: 0.0005386266275309026 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 9969, loss: 0.0005718670436181128 learning_rate: 1e-05\n",
      "step: 9979, loss: 0.0005046901060268283 learning_rate: 1e-05\n",
      "step: 9989, loss: 0.0005128485499881208 learning_rate: 1e-05\n",
      "step: 9999, loss: 0.0004357931320555508 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10009, loss: 0.000594030600041151 learning_rate: 1e-05\n",
      "step: 10019, loss: 0.0006852351361885667 learning_rate: 1e-05\n",
      "step: 10029, loss: 0.0018408847972750664 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 10039, loss: 0.0003798087709583342 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 10049, loss: 0.0009247100679203868 learning_rate: 1e-05\n",
      "step: 10059, loss: 0.00045621584285981953 learning_rate: 1e-05\n",
      "step: 10069, loss: 0.0004625694709829986 learning_rate: 1e-05\n",
      "step: 10079, loss: 0.0004597461665980518 learning_rate: 1e-05\n",
      "step: 10089, loss: 0.0004237465327605605 learning_rate: 1e-05\n",
      "step: 10099, loss: 0.0004384592466522008 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10109, loss: 0.00038640815182588995 learning_rate: 1e-05\n",
      "step: 10119, loss: 0.0004680848214775324 learning_rate: 1e-05\n",
      "step: 10129, loss: 0.0004765839548781514 learning_rate: 1e-05\n",
      "step: 10139, loss: 0.00045033544301986694 learning_rate: 1e-05\n",
      "step: 10149, loss: 0.0005229964153841138 learning_rate: 1e-05\n",
      "step: 10159, loss: 0.00042702234350144863 learning_rate: 1e-05\n",
      "step: 10169, loss: 0.0004845312796533108 learning_rate: 1e-05\n",
      "step: 10179, loss: 0.0004051536088809371 learning_rate: 1e-05\n",
      "step: 10189, loss: 0.0004140329547226429 learning_rate: 1e-05\n",
      "step: 10199, loss: 0.0003906757920049131 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10209, loss: 0.0005095952656120062 learning_rate: 1e-05\n",
      "step: 10219, loss: 0.0007854295545257628 learning_rate: 1e-05\n",
      "step: 10229, loss: 0.00047756798448972404 learning_rate: 1e-05\n",
      "step: 10239, loss: 0.0004173805355094373 learning_rate: 1e-05\n",
      "step: 10249, loss: 0.00039141421439126134 learning_rate: 1e-05\n",
      "step: 10259, loss: 0.000403612240916118 learning_rate: 1e-05\n",
      "step: 10269, loss: 0.0010299463756382465 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 10279, loss: 0.0004422820929903537 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 10289, loss: 0.00039004586869850755 learning_rate: 1e-05\n",
      "step: 10299, loss: 0.0004656949604395777 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10309, loss: 0.0004473727021832019 learning_rate: 1e-05\n",
      "step: 10319, loss: 0.00041592164780013263 learning_rate: 1e-05\n",
      "step: 10329, loss: 0.0004492405569180846 learning_rate: 1e-05\n",
      "step: 10339, loss: 0.0003862131852656603 learning_rate: 1e-05\n",
      "step: 10349, loss: 0.000462877273093909 learning_rate: 1e-05\n",
      "step: 10359, loss: 0.0004776426940225065 learning_rate: 1e-05\n",
      "step: 10369, loss: 0.0004325459012761712 learning_rate: 1e-05\n",
      "step: 10379, loss: 0.00041532490286044776 learning_rate: 1e-05\n",
      "step: 10389, loss: 0.0005369619466364384 learning_rate: 1e-05\n",
      "step: 10399, loss: 0.0007636197842657566 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10409, loss: 0.0004380323807708919 learning_rate: 1e-05\n",
      "step: 10419, loss: 0.0003438755520619452 learning_rate: 1e-05\n",
      "step: 10429, loss: 0.0004866064409725368 learning_rate: 1e-05\n",
      "step: 10439, loss: 0.000377207703422755 learning_rate: 1e-05\n",
      "step: 10449, loss: 0.0004216496308799833 learning_rate: 1e-05\n",
      "step: 10459, loss: 0.000945017789490521 learning_rate: 1e-05\n",
      "step: 10469, loss: 0.0005701147019863129 learning_rate: 1e-05\n",
      "step: 10479, loss: 0.0004528691933955997 learning_rate: 1e-05\n",
      "step: 10489, loss: 0.0004766272322740406 learning_rate: 1e-05\n",
      "step: 10499, loss: 0.0008339915657415986 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10509, loss: 0.0003948946250602603 learning_rate: 1e-05\n",
      "step: 10519, loss: 0.0003961163747590035 learning_rate: 1e-05\n",
      "step: 10529, loss: 0.0003807025495916605 learning_rate: 1e-05\n",
      "step: 10539, loss: 0.0003580274642445147 learning_rate: 1e-05\n",
      "step: 10549, loss: 0.000489469151943922 learning_rate: 1e-05\n",
      "step: 10559, loss: 0.0004131879541091621 learning_rate: 1e-05\n",
      "step: 10569, loss: 0.0004906805697828531 learning_rate: 1e-05\n",
      "step: 10579, loss: 0.00044715357944369316 learning_rate: 1e-05\n",
      "step: 10589, loss: 0.000359653786290437 learning_rate: 1e-05\n",
      "step: 10599, loss: 0.0005947368917986751 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10609, loss: 0.0006961252074688673 learning_rate: 1e-05\n",
      "step: 10619, loss: 0.0014448536094278097 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 10629, loss: 0.0003238923673052341 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 10639, loss: 0.00038125988794490695 learning_rate: 1e-05\n",
      "step: 10649, loss: 0.0023584053851664066 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 10659, loss: 0.0003657456545624882 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 10669, loss: 0.0004800699825864285 learning_rate: 1e-05\n",
      "step: 10679, loss: 0.0004454568261280656 learning_rate: 1e-05\n",
      "step: 10689, loss: 0.000494330539368093 learning_rate: 1e-05\n",
      "step: 10699, loss: 0.0004423710342962295 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10709, loss: 0.00048818992218002677 learning_rate: 1e-05\n",
      "step: 10719, loss: 0.00045261962804943323 learning_rate: 1e-05\n",
      "step: 10729, loss: 0.000611020834185183 learning_rate: 1e-05\n",
      "step: 10739, loss: 0.0004464536323212087 learning_rate: 1e-05\n",
      "step: 10749, loss: 0.0003700640518218279 learning_rate: 1e-05\n",
      "step: 10759, loss: 0.0007574318442493677 learning_rate: 1e-05\n",
      "step: 10769, loss: 0.0005899566458538175 learning_rate: 1e-05\n",
      "step: 10779, loss: 0.00040921662002801895 learning_rate: 1e-05\n",
      "step: 10789, loss: 0.00041219283593818545 learning_rate: 1e-05\n",
      "step: 10799, loss: 0.0005660044262185693 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10809, loss: 0.0006980772595852613 learning_rate: 1e-05\n",
      "step: 10819, loss: 0.0003524160129018128 learning_rate: 1e-05\n",
      "step: 10829, loss: 0.0005951428902335465 learning_rate: 1e-05\n",
      "step: 10839, loss: 0.0003185969835612923 learning_rate: 1e-05\n",
      "step: 10849, loss: 0.0006147733656689525 learning_rate: 1e-05\n",
      "step: 10859, loss: 0.0004817968583665788 learning_rate: 1e-05\n",
      "step: 10869, loss: 0.0004506331169977784 learning_rate: 1e-05\n",
      "step: 10879, loss: 0.0004207318415865302 learning_rate: 1e-05\n",
      "step: 10889, loss: 0.00044524899567477405 learning_rate: 1e-05\n",
      "step: 10899, loss: 0.00042235624277964234 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 10909, loss: 0.0004943516105413437 learning_rate: 1e-05\n",
      "step: 10919, loss: 0.00037293194327503443 learning_rate: 1e-05\n",
      "step: 10929, loss: 0.0004701275029219687 learning_rate: 1e-05\n",
      "step: 10939, loss: 0.0004925007233396173 learning_rate: 1e-05\n",
      "step: 10949, loss: 0.0005125238094478846 learning_rate: 1e-05\n",
      "step: 10959, loss: 0.0004512780287768692 learning_rate: 1e-05\n",
      "step: 10969, loss: 0.0003857694973703474 learning_rate: 1e-05\n",
      "step: 10979, loss: 0.00042593362741172314 learning_rate: 1e-05\n",
      "step: 10989, loss: 0.0006864788010716438 learning_rate: 1e-05\n",
      "step: 10999, loss: 0.00041666897595860064 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11009, loss: 0.0004353086987975985 learning_rate: 1e-05\n",
      "step: 11019, loss: 0.0009139614412561059 learning_rate: 1e-05\n",
      "step: 11029, loss: 0.000371777918189764 learning_rate: 1e-05\n",
      "step: 11039, loss: 0.00043443983304314315 learning_rate: 1e-05\n",
      "step: 11049, loss: 0.0008354910532943904 learning_rate: 1e-05\n",
      "step: 11059, loss: 0.0005008637672290206 learning_rate: 1e-05\n",
      "step: 11069, loss: 0.00039137963904067874 learning_rate: 1e-05\n",
      "step: 11079, loss: 0.0003875519905705005 learning_rate: 1e-05\n",
      "step: 11089, loss: 0.00040460313903167844 learning_rate: 1e-05\n",
      "step: 11099, loss: 0.0004437167663127184 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11109, loss: 0.0004085218533873558 learning_rate: 1e-05\n",
      "step: 11119, loss: 0.0004312794771976769 learning_rate: 1e-05\n",
      "step: 11129, loss: 0.0004912760923616588 learning_rate: 1e-05\n",
      "step: 11139, loss: 0.0004585798888001591 learning_rate: 1e-05\n",
      "step: 11149, loss: 0.0006218343041837215 learning_rate: 1e-05\n",
      "step: 11159, loss: 0.0004390065441839397 learning_rate: 1e-05\n",
      "step: 11169, loss: 0.00038183428114280105 learning_rate: 1e-05\n",
      "step: 11179, loss: 0.0006930987001396716 learning_rate: 1e-05\n",
      "step: 11189, loss: 0.0004416022275108844 learning_rate: 1e-05\n",
      "step: 11199, loss: 0.00039333608583547175 learning_rate: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 11209, loss: 0.0006929440423846245 learning_rate: 1e-05\n",
      "step: 11219, loss: 0.0003970702819060534 learning_rate: 1e-05\n",
      "step: 11229, loss: 0.0003870262298732996 learning_rate: 1e-05\n",
      "step: 11239, loss: 0.000366187043255195 learning_rate: 1e-05\n",
      "step: 11249, loss: 0.000452393083833158 learning_rate: 1e-05\n",
      "step: 11259, loss: 0.00042907107854261994 learning_rate: 1e-05\n",
      "step: 11269, loss: 0.00041113526094704866 learning_rate: 1e-05\n",
      "step: 11279, loss: 0.0003674817271530628 learning_rate: 1e-05\n",
      "step: 11289, loss: 0.00041824375512078404 learning_rate: 1e-05\n",
      "step: 11299, loss: 0.0005369142745621502 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11309, loss: 0.0004068761772941798 learning_rate: 1e-05\n",
      "step: 11319, loss: 0.0004467683029361069 learning_rate: 1e-05\n",
      "step: 11329, loss: 0.0003724390990100801 learning_rate: 1e-05\n",
      "step: 11339, loss: 0.0004897998878732324 learning_rate: 1e-05\n",
      "step: 11349, loss: 0.00040296337101608515 learning_rate: 1e-05\n",
      "step: 11359, loss: 0.0004929113201797009 learning_rate: 1e-05\n",
      "step: 11369, loss: 0.0011434177868068218 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 11379, loss: 0.00036193581763654947 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 11389, loss: 0.0007527863490395248 learning_rate: 1e-05\n",
      "step: 11399, loss: 0.0007795738056302071 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11409, loss: 0.00042320441571064293 learning_rate: 1e-05\n",
      "step: 11419, loss: 0.00037560646887868643 learning_rate: 1e-05\n",
      "step: 11429, loss: 0.0010383273474872112 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 11439, loss: 0.0005067863967269659 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 11449, loss: 0.000406381645007059 learning_rate: 1e-05\n",
      "step: 11459, loss: 0.0028538908809423447 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 11469, loss: 0.0003919517621397972 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 11479, loss: 0.0004398512828629464 learning_rate: 1e-05\n",
      "step: 11489, loss: 0.0004707455518655479 learning_rate: 1e-05\n",
      "step: 11499, loss: 0.0004597886581905186 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11509, loss: 0.00042098149424418807 learning_rate: 1e-05\n",
      "step: 11519, loss: 0.0013664413709193468 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 11529, loss: 0.0003948365629184991 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 11539, loss: 0.00042872835183516145 learning_rate: 1e-05\n",
      "step: 11549, loss: 0.0003623966476880014 learning_rate: 1e-05\n",
      "step: 11559, loss: 0.0003506498469505459 learning_rate: 1e-05\n",
      "step: 11569, loss: 0.00041295652044937015 learning_rate: 1e-05\n",
      "step: 11579, loss: 0.0010304349707439542 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 11589, loss: 0.00038550145109184086 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 11599, loss: 0.00036045239539816976 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11609, loss: 0.0004489227430894971 learning_rate: 1e-05\n",
      "step: 11619, loss: 0.00038017972838133574 learning_rate: 1e-05\n",
      "step: 11629, loss: 0.000849691336043179 learning_rate: 1e-05\n",
      "step: 11639, loss: 0.0006239874055609107 learning_rate: 1e-05\n",
      "step: 11649, loss: 0.00041713097016327083 learning_rate: 1e-05\n",
      "step: 11659, loss: 0.00040330964839085937 learning_rate: 1e-05\n",
      "step: 11669, loss: 0.00036175939021632075 learning_rate: 1e-05\n",
      "step: 11679, loss: 0.0003452153177931905 learning_rate: 1e-05\n",
      "step: 11689, loss: 0.0004976053023710847 learning_rate: 1e-05\n",
      "step: 11699, loss: 0.0013101408258080482 learning_rate: 1e-05\n",
      "New learning rate\n",
      "Checkpoint is saved\n",
      "step: 11709, loss: 0.0008184374310076237 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 11719, loss: 0.00034306870657019317 learning_rate: 1e-05\n",
      "step: 11729, loss: 0.00037901889299973845 learning_rate: 1e-05\n",
      "step: 11739, loss: 0.0004264080198481679 learning_rate: 1e-05\n",
      "step: 11749, loss: 0.00037269253516569734 learning_rate: 1e-05\n",
      "step: 11759, loss: 0.0004127228748984635 learning_rate: 1e-05\n",
      "step: 11769, loss: 0.0008984734304249287 learning_rate: 1e-05\n",
      "step: 11779, loss: 0.00036997359711676836 learning_rate: 1e-05\n",
      "step: 11789, loss: 0.0004654415533877909 learning_rate: 1e-05\n",
      "step: 11799, loss: 0.0003683713439386338 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11809, loss: 0.0003184506786055863 learning_rate: 1e-05\n",
      "step: 11819, loss: 0.0008294647559523582 learning_rate: 1e-05\n",
      "step: 11829, loss: 0.00043579493649303913 learning_rate: 1e-05\n",
      "step: 11839, loss: 0.0007191681070253253 learning_rate: 1e-05\n",
      "step: 11849, loss: 0.00038903974927961826 learning_rate: 1e-05\n",
      "step: 11859, loss: 0.0008192297536879778 learning_rate: 1e-05\n",
      "step: 11869, loss: 0.0004446236416697502 learning_rate: 1e-05\n",
      "step: 11879, loss: 0.0004287178162485361 learning_rate: 1e-05\n",
      "step: 11889, loss: 0.0007410048274323344 learning_rate: 1e-05\n",
      "step: 11899, loss: 0.0004573458281811327 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 11909, loss: 0.00038487481651827693 learning_rate: 1e-05\n",
      "step: 11919, loss: 0.0003402822185307741 learning_rate: 1e-05\n",
      "step: 11929, loss: 0.0004961033118888736 learning_rate: 1e-05\n",
      "step: 11939, loss: 0.00046275428030639887 learning_rate: 1e-05\n",
      "step: 11949, loss: 0.0003587158862501383 learning_rate: 1e-05\n",
      "step: 11959, loss: 0.0005347381811589003 learning_rate: 1e-05\n",
      "step: 11969, loss: 0.00043454288970679045 learning_rate: 1e-05\n",
      "step: 11979, loss: 0.0006953776464797556 learning_rate: 1e-05\n",
      "step: 11989, loss: 0.0005083371652290225 learning_rate: 1e-05\n",
      "step: 11999, loss: 0.0006704197730869055 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 12009, loss: 0.00038399931509047747 learning_rate: 1e-05\n",
      "step: 12019, loss: 0.0009394724620506167 learning_rate: 1e-05\n",
      "step: 12029, loss: 0.0003796112723648548 learning_rate: 1e-05\n",
      "step: 12039, loss: 0.00041321347816847265 learning_rate: 1e-05\n",
      "step: 12049, loss: 0.0008607238414697349 learning_rate: 1e-05\n",
      "step: 12059, loss: 0.0003637712507043034 learning_rate: 1e-05\n",
      "step: 12069, loss: 0.0008968634065240622 learning_rate: 1e-05\n",
      "step: 12079, loss: 0.0006155415321700275 learning_rate: 1e-05\n",
      "step: 12089, loss: 0.000526083109434694 learning_rate: 1e-05\n",
      "step: 12099, loss: 0.00041247220360673964 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 12109, loss: 0.0004253280349075794 learning_rate: 1e-05\n",
      "step: 12119, loss: 0.0003994639846496284 learning_rate: 1e-05\n",
      "step: 12129, loss: 0.00041114320629276335 learning_rate: 1e-05\n",
      "step: 12139, loss: 0.0004787955549545586 learning_rate: 1e-05\n",
      "step: 12149, loss: 0.0016171090537682176 learning_rate: 1e-05\n",
      "New learning rate\n",
      "step: 12159, loss: 0.0004291023942641914 learning_rate: 0.0001\n",
      "New learning rate\n",
      "step: 12169, loss: 0.00045772953308187425 learning_rate: 1e-05\n",
      "step: 12179, loss: 0.00036794034531340003 learning_rate: 1e-05\n",
      "step: 12189, loss: 0.0003144652582705021 learning_rate: 1e-05\n",
      "step: 12199, loss: 0.0003749329480342567 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 12209, loss: 0.0006108577945269644 learning_rate: 1e-05\n",
      "step: 12219, loss: 0.0004558555083349347 learning_rate: 1e-05\n",
      "step: 12229, loss: 0.0003985087969340384 learning_rate: 1e-05\n",
      "step: 12239, loss: 0.00039525696774944663 learning_rate: 1e-05\n",
      "step: 12249, loss: 0.0003673613246064633 learning_rate: 1e-05\n",
      "step: 12259, loss: 0.0003528923261910677 learning_rate: 1e-05\n",
      "step: 12269, loss: 0.00036125333281233907 learning_rate: 1e-05\n",
      "step: 12279, loss: 0.0003676869673654437 learning_rate: 1e-05\n",
      "step: 12289, loss: 0.0004344186163507402 learning_rate: 1e-05\n",
      "step: 12299, loss: 0.0004321503220126033 learning_rate: 1e-05\n",
      "Checkpoint is saved\n",
      "step: 12309, loss: 0.00044585956493392587 learning_rate: 1e-05\n",
      "step: 12319, loss: 0.00039027229649946094 learning_rate: 1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-b407ff3686c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlearning_rate_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mbackward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m9\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-dc0c3d03eb56>\u001b[0m in \u001b[0;36mbackward_step\u001b[1;34m(sess, optimizer, feed)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbackward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "#tf.trainable_variables())\n",
    "checkpointsPath = './checkpoints501/'\n",
    "restore = True\n",
    "starting_step = 0\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if (restore):\n",
    "        print('Restoring')\n",
    "        with open(checkpointsPath + 'checkpoint') as f:\n",
    "            starting_step = int(re.match('model_checkpoint_path: \"-([0-9]+)\"', list(f)[0]).groups()[0]) + 1\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpointsPath))\n",
    "        print('Running from step {}'.format(starting_step))\n",
    "    else:\n",
    "        print('Running from scratch: generating random model parameters.')\n",
    "        learning_rate = default_learning_rate\n",
    "    \n",
    "    print(\"Starting\")\n",
    "    t = time.time()\n",
    "    for step in range(starting_step, starting_step + steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "        feed[learning_rate_placeholder] = learning_rate\n",
    "            \n",
    "        backward_step(sess, optimizer, feed)\n",
    "        \n",
    "        if step % 10 == 9 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {} learning_rate: {}'.format(step, loss_value, learning_rate))\n",
    "            losses.append(loss_value)\n",
    "            \n",
    "            # training op\n",
    "            new_learning_rate = learning_rate_thresholds[sorted(filter(lambda x: x > loss_value, list(learning_rate_thresholds.keys())))[0]]\n",
    "            if new_learning_rate is not learning_rate:\n",
    "                print('New learning rate')\n",
    "                learning_rate = new_learning_rate\n",
    "            \n",
    "        \n",
    "        if step % 100 == 99:\n",
    "            saver.save(sess, checkpointsPath, global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "view = losses[805:]\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    stepinterval = 10\n",
    "    plt.plot(list(range(0, len(view) * stepinterval, stepinterval)), view, linewidth = 1)\n",
    "    plt.xlabel('Training Progress (steps)'.format(starting_step))\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((-0.01, max(view) + (0.25 * max(view))))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints501/-12299\n",
      "1.\n",
      "--------------------------------\n",
      "तस्मात् ते गत्वा तस्य प्रयोजनीयद्रव्याणि संग्रहीतुं शोमिरोणीयानां ग्रामं प्रविविशुः। <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0938\\u094d\\u092e\\u093e\\u0924\\u094d \\u0924\\u0947 \\u0917\\u0924\\u094d\\u0935\\u093e \\u0924\\u0938\\u094d\\u092f \\u092a\\u094d\\u0930\\u092f\\u094b\\u091c\\u0928\\u0940\\u092f\\u0926\\u094d\\u0930\\u0935\\u094d\\u092f\\u093e\\u0923\\u093f \\u0938\\u0902\\u0917\\u094d\\u0930\\u0939\\u0940\\u0924\\u0941\\u0902 \\u0936\\u094b\\u092e\\u093f\\u0930\\u094b\\u0923\\u0940\\u092f\\u093e\\u0928\\u093e\\u0902 \\u0917\\u094d\\u0930\\u093e\\u092e\\u0902 \\u092a\\u094d\\u0930\\u0935\\u093f\\u0935\\u093f\\u0936\\u0941\\u0903\\u0964 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  and they reasoned for every multitude had prayed <ukn> and they could not his brother was healed <ukn> his blood <ukn> him <ukn> him <ukn> him <ukn> \n",
      "   Expected:  and sent messengers before his face <ukn> and they went <ukn> and entered into a village of the samaritans <ukn> to make ready for him <ukn> \n",
      "--------------------------------\n",
      "2.\n",
      "--------------------------------\n",
      "सर्वभूतस्थमात्मानं सर्वभूतानि चात्मनि <ukn> ईक्षते योगयुक्तात्मा सर्वत्र समदर्शन <ukn> <ukn> <ukn> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0938\\u0930\\u094d\\u0935\\u092d\\u0942\\u0924\\u0938\\u094d\\u0925\\u092e\\u093e\\u0924\\u094d\\u092e\\u093e\\u0928\\u0902 \\u0938\\u0930\\u094d\\u0935\\u092d\\u0942\\u0924\\u093e\\u0928\\u093f \\u091a\\u093e\\u0924\\u094d\\u092e\\u0928\\u093f <ukn> \\u0908\\u0915\\u094d\\u0937\\u0924\\u0947 \\u092f\\u094b\\u0917\\u092f\\u0941\\u0915\\u094d\\u0924\\u093e\\u0924\\u094d\\u092e\\u093e \\u0938\\u0930\\u094d\\u0935\\u0924\\u094d\\u0930 \\u0938\\u092e\\u0926\\u0930\\u094d\\u0936\\u0928 <ukn> <ukn> <ukn> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  and pray over many days <ukn> that they should be received you with the house of many <ukn> many sent down of them <ukn> you <ukn> and them that should \n",
      "   Expected:  the true yogis <ukn> uniting their consciousness with god <ukn> see with equal eye <ukn> all living beings in god and god in all living beings <ukn> \n",
      "--------------------------------\n",
      "3.\n",
      "--------------------------------\n",
      "यतो <ukn> लोकोऽहं यञ्चाहं परिचरामि तदीय एको दूतो ह्यो रात्रौ ममान्तिके तिष्ठन् कथितवान् <ukn> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u092f\\u0924\\u094b <ukn> \\u0932\\u094b\\u0915\\u094b\\u093d\\u0939\\u0902 \\u092f\\u091e\\u094d\\u091a\\u093e\\u0939\\u0902 \\u092a\\u0930\\u093f\\u091a\\u0930\\u093e\\u092e\\u093f \\u0924\\u0926\\u0940\\u092f \\u090f\\u0915\\u094b \\u0926\\u0942\\u0924\\u094b \\u0939\\u094d\\u092f\\u094b \\u0930\\u093e\\u0924\\u094d\\u0930\\u094c \\u092e\\u092e\\u093e\\u0928\\u094d\\u0924\\u093f\\u0915\\u0947 \\u0924\\u093f\\u0937\\u094d\\u0920\\u0928\\u094d \\u0915\\u0925\\u093f\\u0924\\u0935\\u093e\\u0928\\u094d <ukn> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  then said the king of the jews <ukn> but afterward thou to live now as now <ukn> <ukn> <ukn> as i now <ukn> adultery the way of the \n",
      "   Expected:  for there stood by me this night the angel of god <ukn> whose i am <ukn> and whom i serve <ukn> \n",
      "--------------------------------\n",
      "4.\n",
      "--------------------------------\n",
      "हे प्रभो <ukn> मदीय एको दासः <ukn> भृशं व्यथितः <ukn> सतु शयनीय आस्ते। <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0939\\u0947 \\u092a\\u094d\\u0930\\u092d\\u094b <ukn> \\u092e\\u0926\\u0940\\u092f \\u090f\\u0915\\u094b \\u0926\\u093e\\u0938\\u0903 <ukn> \\u092d\\u0943\\u0936\\u0902 \\u0935\\u094d\\u092f\\u0925\\u093f\\u0924\\u0903 <ukn> \\u0938\\u0924\\u0941 \\u0936\\u092f\\u0928\\u0940\\u092f \\u0906\\u0938\\u094d\\u0924\\u0947\\u0964 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  and he saith unto him <ukn> let it alone thou for the father <ukn> for it is it <ukn> for a sword <ukn> him <ukn> him <ukn> him \n",
      "   Expected:  and saying <ukn> lord <ukn> my servant lieth at home sick of the palsy <ukn> grievously tormented <ukn> \n",
      "--------------------------------\n",
      "5.\n",
      "--------------------------------\n",
      "यान् एतान् मनुष्यान् यूयमत्र समानयत ते <ukn> युष्माकं देव्या निन्दकाश्च न भवन्ति। <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u092f\\u093e\\u0928\\u094d \\u090f\\u0924\\u093e\\u0928\\u094d \\u092e\\u0928\\u0941\\u0937\\u094d\\u092f\\u093e\\u0928\\u094d \\u092f\\u0942\\u092f\\u092e\\u0924\\u094d\\u0930 \\u0938\\u092e\\u093e\\u0928\\u092f\\u0924 \\u0924\\u0947 <ukn> \\u092f\\u0941\\u0937\\u094d\\u092e\\u093e\\u0915\\u0902 \\u0926\\u0947\\u0935\\u094d\\u092f\\u093e \\u0928\\u093f\\u0928\\u094d\\u0926\\u0915\\u093e\\u0936\\u094d\\u091a \\u0928 \\u092d\\u0935\\u0928\\u094d\\u0924\\u093f\\u0964 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  that they have eaten with me <ukn> and they that have been written <ukn> then shall they be crucified your work <ukn> your peace <ukn> your peace <ukn> the \n",
      "   Expected:  for ye have brought hither these men <ukn> which are neither robbers of churches <ukn> nor yet blasphemers of your goddess <ukn> \n",
      "--------------------------------\n",
      "6.\n",
      "--------------------------------\n",
      "ततः सोवादीद् <ukn> यूयं न शृणुथ तर्हि कुतः पुनः श्रोतुम् इच्छथ <ukn> यूयमपि किं तस्य शिष्या भवितुम् इच्छथ <ukn> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0924\\u0903 \\u0938\\u094b\\u0935\\u093e\\u0926\\u0940\\u0926\\u094d <ukn> \\u092f\\u0942\\u092f\\u0902 \\u0928 \\u0936\\u0943\\u0923\\u0941\\u0925 \\u0924\\u0930\\u094d\\u0939\\u093f \\u0915\\u0941\\u0924\\u0903 \\u092a\\u0941\\u0928\\u0903 \\u0936\\u094d\\u0930\\u094b\\u0924\\u0941\\u092e\\u094d \\u0907\\u091a\\u094d\\u091b\\u0925 <ukn> \\u092f\\u0942\\u092f\\u092e\\u092a\\u093f \\u0915\\u093f\\u0902 \\u0924\\u0938\\u094d\\u092f \\u0936\\u093f\\u0937\\u094d\\u092f\\u093e \\u092d\\u0935\\u093f\\u0924\\u0941\\u092e\\u094d \\u0907\\u091a\\u094d\\u091b\\u0925 <ukn> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  and jesus saith unto them <ukn> receive ye not <ukn> ye will not put him <ukn> on your disciples <ukn> peter also <ukn> and christ <ukn> his disciples \n",
      "   Expected:  he answered them <ukn> i have told you already <ukn> and ye did not hear <ukn> wherefore would ye hear it again <ukn> will ye also be his disciples <ukn> \n",
      "--------------------------------\n",
      "7.\n",
      "--------------------------------\n",
      "<ukn> <ukn> पूर्व्वदिनस्य सायंकाल आगत <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<ukn> <ukn> \\u092a\\u0942\\u0930\\u094d\\u0935\\u094d\\u0935\\u0926\\u093f\\u0928\\u0938\\u094d\\u092f \\u0938\\u093e\\u092f\\u0902\\u0915\\u093e\\u0932 \\u0906\\u0917\\u0924 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  for john was an oath at the jews <ukn> let us hold an melchisedec <ukn> as an oath is said unto him <ukn> an oath of what is without an unclean spirit \n",
      "   Expected:  and now when the even was come <ukn> because it was the preparation <ukn> that is <ukn> the day before the sabbath <ukn> \n",
      "--------------------------------\n",
      "8.\n",
      "--------------------------------\n",
      "यतः केचिदूचुर्योहन् श्मशानादुदतिष्ठत्। केचिदूचुः <ukn> एलियो दर्शनं दत्तवान् <ukn> एवमन्यलोका ऊचुः पूर्व्वीयः कश्चिद् भविष्यद्वादी समुत्थितः। <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u092f\\u0924\\u0903 \\u0915\\u0947\\u091a\\u093f\\u0926\\u0942\\u091a\\u0941\\u0930\\u094d\\u092f\\u094b\\u0939\\u0928\\u094d \\u0936\\u094d\\u092e\\u0936\\u093e\\u0928\\u093e\\u0926\\u0941\\u0926\\u0924\\u093f\\u0937\\u094d\\u0920\\u0924\\u094d\\u0964 \\u0915\\u0947\\u091a\\u093f\\u0926\\u0942\\u091a\\u0941\\u0903 <ukn> \\u090f\\u0932\\u093f\\u092f\\u094b \\u0926\\u0930\\u094d\\u0936\\u0928\\u0902 \\u0926\\u0924\\u094d\\u0924\\u0935\\u093e\\u0928\\u094d <ukn> \\u090f\\u0935\\u092e\\u0928\\u094d\\u092f\\u0932\\u094b\\u0915\\u093e \\u090a\\u091a\\u0941\\u0903 \\u092a\\u0942\\u0930\\u094d\\u0935\\u094d\\u0935\\u0940\\u092f\\u0903 \\u0915\\u0936\\u094d\\u091a\\u093f\\u0926\\u094d \\u092d\\u0935\\u093f\\u0937\\u094d\\u092f\\u0926\\u094d\\u0935\\u093e\\u0926\\u0940 \\u0938\\u092e\\u0941\\u0924\\u094d\\u0925\\u093f\\u0924\\u0903\\u0964 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  and he put in a mountain <ukn> and in the place was put into the temple <ukn> the same place of the streets <ukn> so <ukn> so <ukn> \n",
      "   Expected:  and of some <ukn> that elias had appeared <ukn> and of others <ukn> that one of the old prophets was risen again <ukn> \n",
      "--------------------------------\n",
      "9.\n",
      "--------------------------------\n",
      "अननतरं योहनः शिष्यास्तद्वार्त्तां प्राप्यागत्य तस्य <ukn> श्मशानेऽस्थापयन्। <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0905\\u0928\\u0928\\u0924\\u0930\\u0902 \\u092f\\u094b\\u0939\\u0928\\u0903 \\u0936\\u093f\\u0937\\u094d\\u092f\\u093e\\u0938\\u094d\\u0924\\u0926\\u094d\\u0935\\u093e\\u0930\\u094d\\u0924\\u094d\\u0924\\u093e\\u0902 \\u092a\\u094d\\u0930\\u093e\\u092a\\u094d\\u092f\\u093e\\u0917\\u0924\\u094d\\u092f \\u0924\\u0938\\u094d\\u092f <ukn> \\u0936\\u094d\\u092e\\u0936\\u093e\\u0928\\u0947\\u093d\\u0938\\u094d\\u0925\\u093e\\u092a\\u092f\\u0928\\u094d\\u0964 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  and he was a great multitude <ukn> and touched all his son jesus <ukn> and he came and touched the people <ukn> the people and the chief priests prevailed <ukn> \n",
      "   Expected:  and when his disciples heard of it <ukn> they came and took up his corpse <ukn> and laid it in a tomb <ukn> \n",
      "--------------------------------\n",
      "10.\n",
      "--------------------------------\n",
      "तव पुत्रइति विख्यातो भवितुं न योग्योस्मि च <ukn> मां तव वैतनिकं दासं कृत्वा स्थापय। <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0935 \\u092a\\u0941\\u0924\\u094d\\u0930\\u0907\\u0924\\u093f \\u0935\\u093f\\u0916\\u094d\\u092f\\u093e\\u0924\\u094b \\u092d\\u0935\\u093f\\u0924\\u0941\\u0902 \\u0928 \\u092f\\u094b\\u0917\\u094d\\u092f\\u094b\\u0938\\u094d\\u092e\\u093f \\u091a <ukn> \\u092e\\u093e\\u0902 \\u0924\\u0935 \\u0935\\u0948\\u0924\\u0928\\u093f\\u0915\\u0902 \\u0926\\u093e\\u0938\\u0902 \\u0915\\u0943\\u0924\\u094d\\u0935\\u093e \\u0938\\u094d\\u0925\\u093e\\u092a\\u092f\\u0964 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Translation:  therefore i go to thee <ukn> thy brother be not mine <ukn> and i am not the five hand <ukn> and thy brethren <ukn> not thy record <ukn> me <ukn> \n",
      "   Expected:  and am no more worthy to be called thy son <ukn> make me as one of thy hired servants <ukn> \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = de_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    #let's translate these sentences     \n",
    "#     en_sentences = [\"\\u0907\\u092c\\u094d\\u0930\\u093e\\u0939\\u0940\\u092e\\u0903 \\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u0926\\u093e\\u092f\\u0942\\u0926\\u094d \\u0924\\u0938\\u094d\\u092f \",\\\n",
    "#                     '\\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u092f\\u0940\\u0936\\u0941\\u0916\\u094d\\u0930\\u0940\\u0937\\u094d\\u091f\\u0938\\u094d\\u0924\\u0938\\u094d\\u092f']\n",
    "#     en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    en_sentences_encoded = X_test[10:20]\n",
    "    print(len(X_test))\n",
    "    de_sentences_encoded = Y_test[10:20]\n",
    "    \"\"\"    \n",
    "    additional_en_sentences = ['\\\\u0924\\\\u0924\\\\u0903 <ukn> \\\\u092a\\\\u0930\\\\u0902 \\\\u092a\\\\u0941\\\\u0928\\\\u0930\\\\u0928\\\\u094d\\\\u092f\\\\u094b \\\\u091c\\\\u0928\\\\u094b \\\\u0928\\\\u093f\\\\u0936\\\\u094d\\\\u091a\\\\u093f\\\\u0924\\\\u094d\\\\u092f \\\\u092c\\\\u092d\\\\u093e\\\\u0937\\\\u0947']\n",
    "    additional_en_sentences = [sentence + ' '.join(['<pad>'] * (sentence_size - len(sentence.split()))) for sentence in additional_en_sentences]    \n",
    "    en_sentences_encoded += [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in additional_en_sentences]\n",
    "    additional_de_sentences = ['and about the space of one hour after another confidently affirmed']\n",
    "    additional_de_sentences = [sentence + ' '.join(['<pad>'] * (sentence_size - len(sentence.split()))) for sentence in additional_de_sentences]\n",
    "    de_sentences_encoded += [[de_word2idx.get(word, 0) for word in de_sentence.split()] for de_sentence in additional_de_sentences]\n",
    "    \"\"\"\n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint(checkpointsPath)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([de_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print(\" \". join([convert_sanskrit(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            print(\" \". join([(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            print('Translation: ', end = \" \")\n",
    "            for j in range(len(words)):\n",
    "                if words[j] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print((words[j]), end = \" \")\n",
    "            print()\n",
    "            print('   Expected: ', end = \" \")\n",
    "            for j in range(len(de_sentences_encoded[i])):\n",
    "                if de_idx2word[de_sentences_encoded[i][j]] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print((de_idx2word[de_sentences_encoded[i][j]]), end = \" \")\n",
    "            \n",
    "            print('\\n--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model can be improved by using more training steps, better dataset or even with better selection of hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3528\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<go> a new commandment i give unto you <ukn> that ye love one another <ukn> as i have loved you <ukn> that ye also love one another <ukn> <eos> <pad> <pad> <pad>\n",
      "<go> and he said unto them <ukn> full well ye reject the commandment of god <ukn> that ye may keep your own tradition <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and all the people that came together to that sight <ukn> beholding the things which were done <ukn> smote their breasts <ukn> and returned <ukn> <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and how hear we every man in our own tongue <ukn> wherein we were born <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> howbeit jesus spake of his death <ukn> but they thought that he had spoken of taking of rest in sleep <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and the serpent cast out of his mouth water as a flood after the woman <ukn> that he might cause her to be carried away of the flood <ukn> <eos> <pad>\n",
      "<go> he saith unto them <ukn> how then doth david in spirit call him lord <ukn> saying <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and they said unto him <ukn> where wilt thou that we prepare <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> jesus answered and said unto him <ukn> art thou a master of israel <ukn> and knowest not these things <ukn> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<go> and james the son of zebedee <ukn> and john the brother of james <ukn> and he surnamed them boanerges <ukn> which is <ukn> the sons of thunder <ukn> <eos> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for s in [' '.join([de_idx2word[word] for word in sentence]) for sentence in Y_test[:10]]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
