{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pickle\n",
    "import data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def read_sentences(file_path):\n",
    "\tsentences = []\n",
    "\n",
    "\twith open(file_path, 'r') as reader:\n",
    "\t\tfor s in reader:\n",
    "\t\t\tsentences.append(s.strip())\n",
    "\n",
    "\treturn sentences\n",
    "\n",
    "def iteritems(dic):\n",
    "    return iter([(key, dic[key]) for key in dic])\n",
    "\n",
    "def create_dataset(en_sentences, de_sentences):\n",
    "\n",
    "\ten_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in en_sentences for word in sentence.split())\n",
    "\tde_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in de_sentences for word in sentence.split())\n",
    "\n",
    "\ten_vocab = list(map(lambda x: x[0], sorted(en_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "\tde_vocab = list(map(lambda x: x[0], sorted(de_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "\n",
    "\ten_vocab = en_vocab[:20000]\n",
    "\tde_vocab = de_vocab[:30000]\n",
    "\n",
    "\tstart_idx = 2\n",
    "\ten_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(en_vocab)])\n",
    "\ten_word2idx['<ukn>'] = 0\n",
    "\ten_word2idx['<pad>'] = 1\n",
    "\n",
    "\ten_idx2word = dict([(idx, word) for word, idx in iteritems(en_word2idx)])\n",
    "\n",
    "\n",
    "\tstart_idx = 4\n",
    "\tde_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(de_vocab)])\n",
    "\tde_word2idx['<ukn>'] = 0\n",
    "\tde_word2idx['<go>']  = 1\n",
    "\tde_word2idx['<eos>'] = 2\n",
    "\tde_word2idx['<pad>'] = 3\n",
    "\n",
    "\tde_idx2word = dict([(idx, word) for word, idx in iteritems(de_word2idx)])\n",
    "\n",
    "\tx = [[en_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in en_sentences]\n",
    "\ty = [[de_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in de_sentences]\n",
    "\n",
    "\tX = []\n",
    "\tY = []\n",
    "\tfor i in range(len(x)):\n",
    "\t\tn1 = len(x[i])\n",
    "\t\tn2 = len(y[i])\n",
    "\t\tn = n1 if n1 < n2 else n2 \n",
    "\t\tif abs(n1 - n2) <= 0.3 * n:\n",
    "\t\t\tif n1 <= 15 and n2 <= 15:\n",
    "\t\t\t\tX.append(x[i])\n",
    "\t\t\t\tY.append(y[i])\n",
    "\n",
    "\treturn X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "\twith open(file_path, 'wb') as f:\n",
    "\t\tpickle.dump(obj, f, -1)\n",
    "\n",
    "def main():\n",
    "    en_sentences = read_sentences('./Data/bible.en')\n",
    "    de_sentences = read_sentences('./Data/bible.san')\n",
    "\n",
    "    save_dataset('./Data/bible.pkl', create_dataset(en_sentences, de_sentences))\n",
    "    save_dataset('./Data/bible2.pkl', create_dataset(de_sentences, en_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sanskrit(uni):\n",
    "    a = bytearray(uni, encoding = \"utf-8\").decode('unicode-escape')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f,encoding=\"utf_8\")\n",
    "\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab = read_dataset('./Data/bible2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in Sanskrit - encoded: [6411]\n",
      "Sentence in English - encoded: [3443]\n",
      "Decoded:\n",
      "------------------------\n",
      "तस्य पुत्रो रिहबियाम्  तस्य पुत्रोऽबियः  तस्य पुत्र आसा  । \n",
      "\n",
      "And Solomon begat Roboam  and Roboam begat Abia  and Abia begat Asa  "
     ]
    }
   ],
   "source": [
    "#inspecting data\n",
    "print('Sentence in Sanskrit - encoded:', X[0])\n",
    "print('Sentence in English - encoded:', Y[0])\n",
    "print('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[1])):\n",
    "    print(convert_sanskrit(en_idx2word[X[1][i]]), end = \" \")\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[1])):\n",
    "    print(de_idx2word[Y[1][i]], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 10):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [de_word2idx['<go>']] + y[i] + [de_word2idx['<eos>']] + (length-len(y[i])) * [de_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 10\n",
    "output_seq_len = 12\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "de_vocab_size = len(de_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = de_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = de_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = de_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == de_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(de_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "steps = 200\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this list to plot losses through steps\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "Restoring\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints430/-219\n",
      "Running from step 220\n",
      "step: 224, loss: 0.000778620655182749\n",
      "step: 229, loss: 0.06754908710718155\n",
      "step: 234, loss: 0.7427664995193481\n",
      "step: 239, loss: 0.029178010299801826\n",
      "Checkpoint is saved\n",
      "step: 244, loss: 0.002341830637305975\n",
      "step: 249, loss: 0.004045705311000347\n",
      "step: 254, loss: 0.001612894469872117\n",
      "step: 259, loss: 0.0022631173487752676\n",
      "Checkpoint is saved\n",
      "step: 264, loss: 0.1913626492023468\n",
      "step: 269, loss: 1.1046555042266846\n",
      "step: 274, loss: 0.021367942914366722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0f7e038f4698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mbackward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b47ad7b82838>\u001b[0m in \u001b[0;36mbackward_step\u001b[1;34m(sess, feed)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbackward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1116\u001b[1;33m           \u001b[0mfeed_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mname\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Operation was not named: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"%s:%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mname\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1852\u001b[0m     \u001b[1;34m\"\"\"The full name of this operation.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1854\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1855\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1856\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node_def_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "checkpointsPath = './checkpoints430/'\n",
    "restore = True\n",
    "starting_step = 0\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "with tf.Session() as sess:\n",
    "    if (restore):\n",
    "        print('Restoring')\n",
    "        with open(checkpointsPath + 'checkpoint') as f:\n",
    "            starting_step = int(re.match('model_checkpoint_path: \"-([0-9]+)\"', list(f)[0]).groups()[0]) + 1\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpointsPath))\n",
    "        print('Running from step {}'.format(starting_step))\n",
    "    else:\n",
    "        print('Running from scratch: generating random model parameters.')\n",
    "        sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(starting_step, starting_step + steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, checkpointsPath, global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'restore'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-54403420fb75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'restore'"
     ]
    }
   ],
   "source": [
    "tf.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b631b3ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps (from step {})'.format(starting_step))\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((-1, 50))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints430/-259\n",
      "1.\n",
      "--------------------------------\n",
      "हे मम भ्रातरः  संसारो यदि युष्मान् द्वेष्टि तर्हि तद् आश्चर्य्यं न मन्यध्वं। <pad> <pad>\n",
      "\\u0939\\u0947 \\u092e\\u092e \\u092d\\u094d\\u0930\\u093e\\u0924\\u0930\\u0903  \\u0938\\u0902\\u0938\\u093e\\u0930\\u094b \\u092f\\u0926\\u093f \\u092f\\u0941\\u0937\\u094d\\u092e\\u093e\\u0928\\u094d \\u0926\\u094d\\u0935\\u0947\\u0937\\u094d\\u091f\\u093f \\u0924\\u0930\\u094d\\u0939\\u093f \\u0924\\u0926\\u094d \\u0906\\u0936\\u094d\\u091a\\u0930\\u094d\\u092f\\u094d\\u092f\\u0902 \\u0928 \\u092e\\u0928\\u094d\\u092f\\u0927\\u094d\\u0935\\u0902\\u0964 <pad> <pad>\n",
      "Marvel not  my brethren  if the world hate you hate Marvel not  my brethren  if the world hate you hate \n",
      "--------------------------------\n",
      "2.\n",
      "--------------------------------\n",
      "अपरं तस्मिन् पूर्व्वान्धे जने फिरूशिनां निकटम् आनीते सति फिरूशिनोपि तमपृच्छन् कथं दृष्टिं प्राप्तोसि  <pad>\n",
      "\\u0905\\u092a\\u0930\\u0902 \\u0924\\u0938\\u094d\\u092e\\u093f\\u0928\\u094d \\u092a\\u0942\\u0930\\u094d\\u0935\\u094d\\u0935\\u093e\\u0928\\u094d\\u0927\\u0947 \\u091c\\u0928\\u0947 \\u092b\\u093f\\u0930\\u0942\\u0936\\u093f\\u0928\\u093e\\u0902 \\u0928\\u093f\\u0915\\u091f\\u092e\\u094d \\u0906\\u0928\\u0940\\u0924\\u0947 \\u0938\\u0924\\u093f \\u092b\\u093f\\u0930\\u0942\\u0936\\u093f\\u0928\\u094b\\u092a\\u093f \\u0924\\u092e\\u092a\\u0943\\u091a\\u094d\\u091b\\u0928\\u094d \\u0915\\u0925\\u0902 \\u0926\\u0943\\u0937\\u094d\\u091f\\u093f\\u0902 \\u092a\\u094d\\u0930\\u093e\\u092a\\u094d\\u0924\\u094b\\u0938\\u093f  <pad>\n",
      "They brought to the Pharisees him that aforetime was blind  They brought to the Pharisees him that aforetime was blind  \n",
      "--------------------------------\n",
      "3.\n",
      "--------------------------------\n",
      "ततो यीशुरवदत् त्वया सार्द्धं कथनं करोमि योऽहम् अहमेव स पुरुषः। <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0924\\u094b \\u092f\\u0940\\u0936\\u0941\\u0930\\u0935\\u0926\\u0924\\u094d \\u0924\\u094d\\u0935\\u092f\\u093e \\u0938\\u093e\\u0930\\u094d\\u0926\\u094d\\u0927\\u0902 \\u0915\\u0925\\u0928\\u0902 \\u0915\\u0930\\u094b\\u092e\\u093f \\u092f\\u094b\\u093d\\u0939\\u092e\\u094d \\u0905\\u0939\\u092e\\u0947\\u0935 \\u0938 \\u092a\\u0941\\u0930\\u0941\\u0937\\u0903\\u0964 <pad> <pad> <pad> <pad> <pad>\n",
      "Jesus saith unto her  I that speak unto thee am am Jesus saith unto her  I that speak unto thee am am \n",
      "--------------------------------\n",
      "4.\n",
      "--------------------------------\n",
      "तस्माद् भवतः शिष्याणां समीपे तमानयं किन्तु ते तं स्वास्थं कर्त्तुं न शक्ताः। <pad> <pad> <pad>\n",
      "\\u0924\\u0938\\u094d\\u092e\\u093e\\u0926\\u094d \\u092d\\u0935\\u0924\\u0903 \\u0936\\u093f\\u0937\\u094d\\u092f\\u093e\\u0923\\u093e\\u0902 \\u0938\\u092e\\u0940\\u092a\\u0947 \\u0924\\u092e\\u093e\\u0928\\u092f\\u0902 \\u0915\\u093f\\u0928\\u094d\\u0924\\u0941 \\u0924\\u0947 \\u0924\\u0902 \\u0938\\u094d\\u0935\\u093e\\u0938\\u094d\\u0925\\u0902 \\u0915\\u0930\\u094d\\u0924\\u094d\\u0924\\u0941\\u0902 \\u0928 \\u0936\\u0915\\u094d\\u0924\\u093e\\u0903\\u0964 <pad> <pad> <pad>\n",
      "And I brought him to thy disciples  and they could they And I brought him to thy disciples  and they could they \n",
      "--------------------------------\n",
      "5.\n",
      "--------------------------------\n",
      "ततो यदि तेन लिखितवानि न प्रतिथ तर्हि मम वाक्यानि कथं प्रत्येष्यथ  <pad> <pad> <pad>\n",
      "\\u0924\\u0924\\u094b \\u092f\\u0926\\u093f \\u0924\\u0947\\u0928 \\u0932\\u093f\\u0916\\u093f\\u0924\\u0935\\u093e\\u0928\\u093f \\u0928 \\u092a\\u094d\\u0930\\u0924\\u093f\\u0925 \\u0924\\u0930\\u094d\\u0939\\u093f \\u092e\\u092e \\u0935\\u093e\\u0915\\u094d\\u092f\\u093e\\u0928\\u093f \\u0915\\u0925\\u0902 \\u092a\\u094d\\u0930\\u0924\\u094d\\u092f\\u0947\\u0937\\u094d\\u092f\\u0925  <pad> <pad> <pad>\n",
      "But if ye believe not his writings  how shall ye shall But if ye believe not his writings  how shall ye shall \n",
      "--------------------------------\n",
      "6.\n",
      "--------------------------------\n",
      "शेषे शौलं मृगयितुं बर्णब्बास्तार्षनगरं प्रस्थितवान्। तत्र तस्योद्देशं प्राप्य तम् आन्तियखियानगरम् आनयत्  <pad> <pad> <pad>\n",
      "\\u0936\\u0947\\u0937\\u0947 \\u0936\\u094c\\u0932\\u0902 \\u092e\\u0943\\u0917\\u092f\\u093f\\u0924\\u0941\\u0902 \\u092c\\u0930\\u094d\\u0923\\u092c\\u094d\\u092c\\u093e\\u0938\\u094d\\u0924\\u093e\\u0930\\u094d\\u0937\\u0928\\u0917\\u0930\\u0902 \\u092a\\u094d\\u0930\\u0938\\u094d\\u0925\\u093f\\u0924\\u0935\\u093e\\u0928\\u094d\\u0964 \\u0924\\u0924\\u094d\\u0930 \\u0924\\u0938\\u094d\\u092f\\u094b\\u0926\\u094d\\u0926\\u0947\\u0936\\u0902 \\u092a\\u094d\\u0930\\u093e\\u092a\\u094d\\u092f \\u0924\\u092e\\u094d \\u0906\\u0928\\u094d\\u0924\\u093f\\u092f\\u0916\\u093f\\u092f\\u093e\\u0928\\u0917\\u0930\\u092e\\u094d \\u0906\\u0928\\u092f\\u0924\\u094d  <pad> <pad> <pad>\n",
      "Then departed Barnabas to Tarsus  for to seek Saul  seek Then departed Barnabas to Tarsus  for to seek Saul  seek \n",
      "--------------------------------\n",
      "7.\n",
      "--------------------------------\n",
      "ततः सर्व्वे लोकास्तं गमनागमने कुर्व्वन्तम् ईश्वरं धन्यं वदन्तञ्च विलोक्य <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0924\\u0903 \\u0938\\u0930\\u094d\\u0935\\u094d\\u0935\\u0947 \\u0932\\u094b\\u0915\\u093e\\u0938\\u094d\\u0924\\u0902 \\u0917\\u092e\\u0928\\u093e\\u0917\\u092e\\u0928\\u0947 \\u0915\\u0941\\u0930\\u094d\\u0935\\u094d\\u0935\\u0928\\u094d\\u0924\\u092e\\u094d \\u0908\\u0936\\u094d\\u0935\\u0930\\u0902 \\u0927\\u0928\\u094d\\u092f\\u0902 \\u0935\\u0926\\u0928\\u094d\\u0924\\u091e\\u094d\\u091a \\u0935\\u093f\\u0932\\u094b\\u0915\\u094d\\u092f <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "And all the people saw him walking and praising God  and And all the people saw him walking and praising God  and \n",
      "--------------------------------\n",
      "8.\n",
      "--------------------------------\n",
      "ततो लोका उच्चैःकारं प्रत्यवदन्  एष मनुजरवो न हि  ईश्वरीयरवः। <pad> <pad> <pad> <pad>\n",
      "\\u0924\\u0924\\u094b \\u0932\\u094b\\u0915\\u093e \\u0909\\u091a\\u094d\\u091a\\u0948\\u0903\\u0915\\u093e\\u0930\\u0902 \\u092a\\u094d\\u0930\\u0924\\u094d\\u092f\\u0935\\u0926\\u0928\\u094d  \\u090f\\u0937 \\u092e\\u0928\\u0941\\u091c\\u0930\\u0935\\u094b \\u0928 \\u0939\\u093f  \\u0908\\u0936\\u094d\\u0935\\u0930\\u0940\\u092f\\u0930\\u0935\\u0903\\u0964 <pad> <pad> <pad> <pad>\n",
      "But the word of God grew and multiplied  shall God But the word of God grew and multiplied  shall God \n",
      "--------------------------------\n",
      "9.\n",
      "--------------------------------\n",
      "परेशानुग्रहे कालं प्रचारयितुमेव च। सर्व्वैतत्करणार्थाय मामेव प्रहिणोति सः॥ <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\\u092a\\u0930\\u0947\\u0936\\u093e\\u0928\\u0941\\u0917\\u094d\\u0930\\u0939\\u0947 \\u0915\\u093e\\u0932\\u0902 \\u092a\\u094d\\u0930\\u091a\\u093e\\u0930\\u092f\\u093f\\u0924\\u0941\\u092e\\u0947\\u0935 \\u091a\\u0964 \\u0938\\u0930\\u094d\\u0935\\u094d\\u0935\\u0948\\u0924\\u0924\\u094d\\u0915\\u0930\\u0923\\u093e\\u0930\\u094d\\u0925\\u093e\\u092f \\u092e\\u093e\\u092e\\u0947\\u0935 \\u092a\\u094d\\u0930\\u0939\\u093f\\u0923\\u094b\\u0924\\u093f \\u0938\\u0903\\u0965 <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "To preach the acceptable year of the Lord  of seen To preach the acceptable year of the Lord  of seen \n",
      "--------------------------------\n",
      "10.\n",
      "--------------------------------\n",
      "यः कश्चित् <ukn> व्यवस्थाम् अवमन्यते स दयां विना <ukn> वा <ukn> <ukn> <ukn>  <pad>\n",
      "\\u092f\\u0903 \\u0915\\u0936\\u094d\\u091a\\u093f\\u0924\\u094d <ukn> \\u0935\\u094d\\u092f\\u0935\\u0938\\u094d\\u0925\\u093e\\u092e\\u094d \\u0905\\u0935\\u092e\\u0928\\u094d\\u092f\\u0924\\u0947 \\u0938 \\u0926\\u092f\\u093e\\u0902 \\u0935\\u093f\\u0928\\u093e <ukn> \\u0935\\u093e <ukn> <ukn> <ukn>  <pad>\n",
      "He that despised Moses ' law died without mercy under two died He that despised Moses ' law died without mercy under two died \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = de_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    #let's translate these sentences     \n",
    "#     en_sentences = [\"\\u0907\\u092c\\u094d\\u0930\\u093e\\u0939\\u0940\\u092e\\u0903 \\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u0926\\u093e\\u092f\\u0942\\u0926\\u094d \\u0924\\u0938\\u094d\\u092f \",\\\n",
    "#                     '\\u0938\\u0928\\u094d\\u0924\\u093e\\u0928\\u094b \\u092f\\u0940\\u0936\\u0941\\u0916\\u094d\\u0930\\u0940\\u0937\\u094d\\u091f\\u0938\\u094d\\u0924\\u0938\\u094d\\u092f']\n",
    "#     en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    en_sentences_encoded = X_train[:10]\n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint(checkpointsPath)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([de_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print(\" \". join([convert_sanskrit(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            print(\" \". join([(en_idx2word[word]) for word in en_sentences_encoded[i]]))\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print((words[i]), end = \" \")\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print(words[i], end = \" \")\n",
    "            \n",
    "            print('\\n--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model can be improved by using more training steps, better dataset or even with better selection of hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
